{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn SVR performance reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Test sklearn SVR performance for referencing \"\"\"\n",
    "import warnings\n",
    "import sklearn.svm as SKSVM\n",
    "import sklearn.model_selection as SK_model_selection\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def mean_absolute_epsilon_error(y_truth, y_pred, epsilon=0, sample_weights=None):\n",
    "    errors = np.absolute(y_pred - y_truth) - epsilon\n",
    "    errors = np.maximum(errors, 0) \n",
    "    return np.average(errors, weights=sample_weights)\n",
    "\n",
    "def load_dataset(train_filepath, test_filepath, classification=True):\n",
    "    \"\"\" Load training, test dataset only in csv \"\"\"\n",
    "\n",
    "    train_data = np.genfromtxt(train_filepath, delimiter=',')\n",
    "    train_X = train_data[:,1:]\n",
    "    train_y = train_data[:,0]\n",
    "\n",
    "    test_data = np.genfromtxt(test_filepath, delimiter=',')\n",
    "    test_X = test_data[:,1:]\n",
    "    test_y = test_data[:,0]\n",
    "    \n",
    "    data = { 'train_X': train_X, 'train_y': train_y, 'test_X': test_X, 'test_y': test_y }\n",
    "    \n",
    "    if classification:\n",
    "        unnormalized_train_test_y = train_y.tolist() + test_y.tolist() # concatenate\n",
    "        train_test_y = normalize_binary_class_label(unnormalized_train_test_y) # normalize binary label to {+1, -1}\n",
    "        train_y = train_test_y[:len(train_y)] # split to train_y\n",
    "        test_y = train_test_y[len(train_y):] # split to test_y\n",
    "\n",
    "        # save the label mapping\n",
    "        binary_class_label = list(set(unnormalized_train_test_y))\n",
    "        if type(binary_class_label[0]) == float:\n",
    "            binary_class_label[0] = int(binary_class_label[0])\n",
    "            binary_class_label[1] = int(binary_class_label[1])\n",
    "        \n",
    "        data['+1'] = binary_class_label[0]\n",
    "        data['-1'] = binary_class_label[1]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Load regression dataset \"\"\"\n",
    "data = load_dataset(train_filepath='dataset/airfoil_self_noise_training.csv',\n",
    "                    test_filepath='dataset/airfoil_self_noise_testing.csv',\n",
    "                    classification=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1002, 5)\n",
      "(1002,)\n",
      "(501, 5)\n",
      "(501,)\n"
     ]
    }
   ],
   "source": [
    "print(data['train_X'].shape)\n",
    "print(data['train_y'].shape)\n",
    "print(data['test_X'].shape)\n",
    "print(data['test_y'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean abs error:  4.79698168323\n",
      "mean abs eps error:  4.79698168323\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Test sklearn SVR performance (linear) \"\"\"\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    clf = SKSVM.SVR(C=1e-5, epsilon=1e-2, kernel='linear', max_iter=5000)\n",
    "    clf.fit(data['train_X'], data['train_y'])\n",
    "    \n",
    "    y_pred = clf.predict(data['test_X'])\n",
    "    \n",
    "    print('mean abs error: ', mean_absolute_error(data['test_y'], y_pred))\n",
    "    print('mean abs eps error: ', mean_absolute_epsilon_error(data['test_y'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean abs error:  5.28101911143\n",
      "mean abs eps error:  5.28101911143\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Test sklearn SVR performance (poly) \"\"\"\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    clf = SKSVM.SVR(C=1e-20, epsilon=1e-2, kernel='poly', max_iter=5000)\n",
    "    clf.fit(data['train_X'], data['train_y'])\n",
    "    \n",
    "    y_pred = clf.predict(data['test_X'])\n",
    "    \n",
    "    print('mean abs error: ', mean_absolute_error(data['test_y'], y_pred))\n",
    "    print('mean abs eps error: ', mean_absolute_epsilon_error(data['test_y'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean abs error:  5.3947313001\n",
      "mean abs eps error:  5.3947313001\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Test sklearn SVR performance (rbf) \"\"\"\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    clf = SKSVM.SVR(C=1e-5, epsilon=1e-2, kernel='rbf', max_iter=5000)\n",
    "    clf.fit(data['train_X'], data['train_y'])\n",
    "    \n",
    "    y_pred = clf.predict(data['test_X'])\n",
    "    \n",
    "    print('mean abs error: ', mean_absolute_error(data['test_y'], y_pred))\n",
    "    print('mean abs eps error: ', mean_absolute_epsilon_error(data['test_y'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Start to code our SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MeanAbsoluteEpsilonError:\n",
    "    def __init__(self, epsilon=0):\n",
    "        self.epsilon = epsilon\n",
    "    def __call__(self, y_truth, y_pred, sample_weights=None):\n",
    "        errors = np.absolute(y_pred - y_truth) - self.epsilon\n",
    "        errors = np.maximum(errors, 0) \n",
    "        return np.average(errors, weights=sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Author: Howard (Yu-Chun) Lo \"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class SVR:\n",
    "    def __init__(self, data, param={}, scorer=mean_absolute_epsilon_error, enable_heuristic=False, enable_kernel_cache=True, max_iteration=3000, verbose=False):\n",
    "        \"\"\" Support vector regression optimized with SMO algorithm\n",
    "\n",
    "            Implementation Reference:\n",
    "                - SMO supplement provided by EE6550 course\n",
    "                - Standford CS 229, Autumn 2009 The Simplified SMO Algorithm\n",
    "                - Improvment to Platt's SMO Algorithm for SVM Classifer Design, S.S. Keerthi et al\n",
    "                - A Roadmap to SVM Sequential Minimal Optimization for Classification by Ruben Ramirez-Padron\n",
    "\n",
    "            @param data: Dictionary of training and test data:\n",
    "                - data['train_X']: Training data with shape(N, D), where N is the number of examples, D is data dimension.\n",
    "                - data['train_y']: Training label with shape(N,)\n",
    "                - data['test_X']: Test data with shape(N, D), where N is the number of examples, D is data dimension.\n",
    "                - data['test_y']: Test label with shape(N,)\n",
    "\n",
    "            @param param: Dictionary of hyper-parameters:\n",
    "                - param['C']: Parameter for penalty term. (Default: 0.1)\n",
    "                - param['epsilon']: Epsilon in the epsilon-SVR model (Default: 0.1)\n",
    "                - param['tol']: Tolerance for KKT conditons. (Defualt: 1e-2)\n",
    "                - param['kernel_type']: Kernel type to be used in SVM. Acceptable kernel type: 'linear', 'poly', 'rbf'. (Default: 'linear')\n",
    "                - param['poly_degree']: Degree of the polynomial kernel function ('poly'). Ignored by all other kernels. (Default: 3)\n",
    "                - param['rbf_sigma']: Sigma term in RBF (guassian). Ignored by all other kernels. (Default: 0.5)\n",
    "            \n",
    "            @param `scorer`: For monitoring error during training. (Default: mean_absolute_epsilon_error)\n",
    "            @param `enable_heuristic`: Whether use Platts heuristics to train SVM. (Defualt: False)\n",
    "            @param `enable_kernel_cache`: Whether precompute kernel results. This can speed up training but need time to initialize when data is large. (Defualt: True)\n",
    "            @param `max_iteration`: Max iteration for SMO training algorithm to avoid not converging. (Default: 5000)\n",
    "        \"\"\"\n",
    "        # Upack training and test data\n",
    "        self.train_X = data['train_X']\n",
    "        self.train_y = data['train_y']\n",
    "        self.test_X = data['test_X']\n",
    "        self.test_y = data['test_y']\n",
    "\n",
    "        # Unpack hyper-parameters\n",
    "        self.C = param.get('C', 0.1)\n",
    "        self.tol = param.get('tol', 1e-2)\n",
    "        self.epsilon = param.get('epsilon', 1e-1)\n",
    "        self.kernel_type = param.get('kernel_type', 'linear')\n",
    "        self.poly_degree = param.get('poly_degree', 3)\n",
    "        self.rbf_sigma = param.get('rbf_sigma', 0.5)\n",
    "\n",
    "        self.scorer = scorer\n",
    "        self.enable_heuristic = enable_heuristic\n",
    "        self.enable_kernel_cache = enable_kernel_cache\n",
    "        self.max_iteration = max_iteration\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Set kernel function\n",
    "        self.kernels = {\n",
    "            'linear': self._linear_kernel,\n",
    "            'poly': self._poly_kernel,\n",
    "            'rbf': self._rbf_kernel\n",
    "        }\n",
    "        self.kernel = self.kernels[self.kernel_type]\n",
    "\n",
    "        # Precompute kernel cache\n",
    "        if self.enable_kernel_cache:\n",
    "            print('-'*100)\n",
    "            print('[*] Enable kernel cache. Precomputing kernel results for all training examples ...')\n",
    "            self.kernel_cache = self._precompute_kernel_cache()\n",
    "\n",
    "        # Model parameters\n",
    "        self.use_w = True if self.kernel_type == 'linear' else False # If linear kernel is used, use weight to perform prediction instead of alphas.\n",
    "        self.w = np.zeros(self.train_X.shape[1]) # Weight vector: shape(D,) this will be updated when training.\n",
    "        self.b = 0.0 # Bias term: scalar, this will be updated when training.\n",
    "        \n",
    "        # SVR needs a pair of lagrange multipliers\n",
    "        # This alpha is not the same as SVC, where each example has one lagrange multiplier `alpha`. \n",
    "        # In SVR, each example has two lagrange multipliers `a1` and `a2`.\n",
    "        # Here, alpha is actually ( a2 - a1 ).\n",
    "        self.alpha = np.zeros(len(self.train_X))\n",
    "\n",
    "        # After training, we can compute biases for support vectors (training examples which 0 < alpha < C)\n",
    "        # for estimating sample mean and sample std of biases.\n",
    "        # For a good learning result, sample std of biases should be small.\n",
    "        self.postcomputed_biases = np.array([None]*len(self.train_X))\n",
    "        self.b_mean = None # Instead of using self.b to make prediction, use self.b_mean when training is done.\n",
    "        self.b_std = None\n",
    "\n",
    "    def train(self, info=''):\n",
    "        \"\"\" Optimize alpha with either simple SMO algorithm or simple SMO combined with Platt's heuristics\n",
    "            In each iteration, the SMO algorithm solves the Lagrangian dual problem\n",
    "            which involves only two Lagrangian multipliers.\n",
    "        \"\"\"\n",
    "        if self.enable_heuristic:\n",
    "            self._heuristic_smo(info)\n",
    "        else:\n",
    "            self._simple_smo(info)\n",
    "\n",
    "    def hypothesis(self, X):\n",
    "        \"\"\" Applying our linear classifier `f(x)` to perform binary classification.\n",
    "            If f(x) >= 0, y(i) = +1\n",
    "            Else    <  0, y(i) = -1\n",
    "\n",
    "            @param `X`: X can be a single example with shape(D,) or multiple examples with shape(N, D)\n",
    "        \"\"\"\n",
    "        # ---- Not the same as SVC ----\n",
    "        return self._f(X)\n",
    "\n",
    "    def _simple_smo(self, info=''):\n",
    "\n",
    "        num_changed_alphas = 1\n",
    "        iteration = 0\n",
    "\n",
    "        while num_changed_alphas > 0:\n",
    "            num_changed_alphas = 0\n",
    "            for i in range(len(self.train_X)):\n",
    "                if self._violate_KKT_conditions(i):\n",
    "                    j = i\n",
    "                    while(j == i): j = np.random.randint(0, len(self.train_X))\n",
    "                    num_changed_alphas += self._update_alpha_pair(i, j)\n",
    "\n",
    "            if self.verbose and num_changed_alphas == 0:\n",
    "                if info: print('[*] {}'.format(info))\n",
    "                print('[*] Converged at iteration {}.'.format(iteration+1))\n",
    "                print('-'*100)\n",
    "\n",
    "            iteration += 1\n",
    "            if self.verbose and (iteration == 1 or iteration % 100 == 0 or iteration == self.max_iteration):\n",
    "                # Compute training and testing error\n",
    "                train_error = self.scorer(y_truth=self.train_y, y_pred=self.hypothesis(X=self.train_X))\n",
    "                test_error = self.scorer(y_truth=self.test_y, y_pred=self.hypothesis(X=self.test_X))\n",
    "                print('-'*100)\n",
    "                if info: print('[*] {}'.format(info))\n",
    "                print('[*] {} alphas changed.'.format(num_changed_alphas))\n",
    "                print('[*] Iteration: {} | Train error: {} | Test error: {}'.format(iteration, train_error, test_error))\n",
    "\n",
    "            if iteration == self.max_iteration:\n",
    "                print('-'*100)\n",
    "                print('[*] Max iteration acheived.')\n",
    "                break\n",
    "\n",
    "        if self.verbose: print('[*] Averaging post-computed biases as final bias of SVM hypothesis.')\n",
    "        self._postcompute_biases()\n",
    "\n",
    "    def _heuristic_smo(self, info=''):\n",
    "\n",
    "        num_changed_alphas = 0\n",
    "        examine_all = 1\n",
    "        iteration = 0\n",
    "\n",
    "        while num_changed_alphas > 0 or examine_all:\n",
    "            num_changed_alphas = 0\n",
    "            if examine_all:\n",
    "                # Repeated pass iterates over entire examples.\n",
    "                for i in range(len(self.train_X)):\n",
    "                    # alpha_i needs update, select alpha_j (!= alpha_i) to jointly optimize the alpha pair\n",
    "                    if self._violate_KKT_conditions(i):\n",
    "                        j = i\n",
    "                        while(j == i): j = np.random.randint(0, len(self.train_X))\n",
    "                        # Update alpha_i and alpha_j\n",
    "                        num_changed_alphas += self._update_alpha_pair(i, j)\n",
    "\n",
    "                if self.verbose:\n",
    "                    print('-'*100)\n",
    "                    if info: print('[*] {}'.format(info))\n",
    "                    print('[*] One passes done.')\n",
    "\n",
    "                if self.verbose and num_changed_alphas == 0:\n",
    "                    if info: print('[*] {}'.format(info))\n",
    "                    print('[*] Converged at iteration {}.'.format(iteration+1))\n",
    "                    print('-'*100)\n",
    "                elif self.verbose:\n",
    "                    if info: print('[*] {}'.format(info))\n",
    "                    print('[*] Go to repeated passes.')\n",
    "            else:\n",
    "                # Repeated pass iterates over non-boundary examples.\n",
    "                I_non_boundary = np.where(np.logical_and(self.alpha > 0, self.alpha < self.C) == True)[0].tolist()\n",
    "                if len(I_non_boundary):\n",
    "                    E_list = np.vectorize(self._E)(I_non_boundary)\n",
    "                    if not max(E_list) - min(E_list) < 1:\n",
    "                        for i in I_non_boundary:\n",
    "                            num_changed_alphas += self._examine_example(i)\n",
    "\n",
    "                if self.verbose and num_changed_alphas == 0:\n",
    "                    print('-'*100)\n",
    "                    if info: print('[*] {}'.format(info))\n",
    "                    print('[*] Repeated passes done. Go back to one pass.')\n",
    "\n",
    "            if examine_all == 1:\n",
    "                # One pass done, go to repeated passes.\n",
    "                examine_all = 0\n",
    "            elif num_changed_alphas == 0:\n",
    "                # Repeated pass done, go back to one pass.\n",
    "                examine_all = 1\n",
    "\n",
    "            iteration += 1\n",
    "            if self.verbose and (iteration == 1 or iteration % 100 == 0 or iteration == self.max_iteration):\n",
    "                # Compute training and testing error\n",
    "                train_error = self.scorer(y_truth=self.train_y, y_pred=self.hypothesis(X=self.train_X))\n",
    "                test_error = self.scorer(y_truth=self.test_y, y_pred=self.hypothesis(X=self.test_X))\n",
    "                print('-'*100)\n",
    "                if info: print('[*] {}'.format(info))\n",
    "                print('[*] {} alphas changed.'.format(num_changed_alphas))\n",
    "                print('[*] Iteration: {} | Train error: {} | Test error: {}'.format(iteration, train_error, test_error))\n",
    "\n",
    "            if iteration == self.max_iteration:\n",
    "                print('-'*100)\n",
    "                print('[*] Max iteration acheived.')\n",
    "                break\n",
    "\n",
    "        if self.verbose: print('[*] Averaging post-computed biases as final bias of SVM hypothesis.')\n",
    "        self._postcompute_biases()\n",
    "\n",
    "    def _violate_KKT_conditions(self, i):\n",
    "        \"\"\" Check if an example violates the KKT conditons \"\"\"\n",
    "        \n",
    "        alpha_i = self.alpha[i]\n",
    "        E_i = self._E(i)\n",
    "        \n",
    "        # ---- Not the same as SVC ----\n",
    "        if alpha_i == 0 and not (-self.epsilon <= E_i and E_i <= self.epsilon):\n",
    "            return True\n",
    "        if (-self.C < alpha_i and alpha_i < 0) and not E_i == self.epsilon:\n",
    "            return True\n",
    "        if (0 < alpha_i and alpha_i < self.C) and not E_i == -self.epsilon:\n",
    "            return True\n",
    "        if alpha_i == -self.C and not E_i >= self.epsilon:\n",
    "            return True\n",
    "        if alpha_i == self.C and not E_i <= self.epsilon:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _examine_example(self, i):\n",
    "        \"\"\" Implement Platt's heuristics to select a good alpha pair to optimize.\n",
    "            (First heuristic is not implemented since it makes training slower)\n",
    "        \"\"\"\n",
    "        # Check if alpha_i needs updating (alpha_i violates KKT conditions)\n",
    "        if self._violate_KKT_conditions(i):\n",
    "\n",
    "            # Retrieve indexes of non boundary examples\n",
    "            I_non_boundary = np.where(np.logical_and(self.alpha > 0, self.alpha < self.C) == True)[0].tolist()\n",
    "\n",
    "            # Iterate over non-boundary items, starting at a random position\n",
    "            shuffled_I_non_boundary = np.copy(I_non_boundary)\n",
    "            np.random.shuffle(shuffled_I_non_boundary)\n",
    "            for j in shuffled_I_non_boundary:\n",
    "                if self._update_alpha_pair(i, j):\n",
    "                    return 1\n",
    "\n",
    "            # Iterate over entire items, starting at a random position\n",
    "            I = np.arange(len(self.train_X))\n",
    "            shuffled_I = np.copy(I)\n",
    "            np.random.shuffle(shuffled_I)\n",
    "            for j in shuffled_I:\n",
    "                if self._update_alpha_pair(i, j):\n",
    "                    return 1\n",
    "        return 0\n",
    "\n",
    "    def _update_alpha_pair(self, i, j):\n",
    "        \"\"\" Jointly optimized alpha_i and alpha_j \"\"\"\n",
    "        # Not the alpha pair.\n",
    "        if i == j: return 0\n",
    "\n",
    "        E_i = self._E(i)\n",
    "        E_j = self._E(j)\n",
    "\n",
    "        alpha_i = self.alpha[i]\n",
    "        alpha_j = self.alpha[j]\n",
    "\n",
    "        x_i, x_j, y_i, y_j = self.train_X[i], self.train_X[j], self.train_y[i], self.train_y[j]\n",
    "        \n",
    "        # ---- Not the same as SVC ----\n",
    "        L = max(-self.C, alpha_i + alpha_j - self.C)\n",
    "        H = min(self.C, alpha_i + alpha_j + self.C)\n",
    "\n",
    "        # This will not make any progress.\n",
    "        if L == H: return 0\n",
    "\n",
    "        # Compute eta (second derivative of the Lagrange dual function = -eta)\n",
    "        if self.enable_kernel_cache:\n",
    "            eta = self.kernel_cache[i][i] + self.kernel_cache[j][j] - 2*self.kernel_cache[i][j]\n",
    "        else:\n",
    "            eta = self.kernel(x_i, x_i) + self.kernel(x_j, x_j) - 2*self.kernel(x_i, x_j)\n",
    "\n",
    "        # eta > 0 => second derivative(-eta) < 0 => maximum exists.\n",
    "        if eta <= 0: return 0\n",
    "    \n",
    "        # ---- Not the same as SVC ----\n",
    "        \n",
    "        # Although the update rule of `alpha_j` is a **function of itself**.\n",
    "        # by analysis, we can still update `alpha_j` by trick, since there's only three possible `alpha_j_new`\n",
    "        # See SMO supplement for more details.\n",
    "        delta_E_ij = E_i - E_j\n",
    "        \n",
    "        # Calculate list of possible new alphas.\n",
    "        # `x` is a function of `alpha_j_new` and it actually only takes one of {-2, 0, 2}\n",
    "        possible_alpha_j_new = lambda x: alpha_j + (delta_E_ij + x*self.epsilon)/eta\n",
    "        possible_alpha_j_new_pos2 = possible_alpha_j_new(2)\n",
    "        possible_alpha_j_new_zero = possible_alpha_j_new(0)\n",
    "        possible_alpha_j_new_neg2 = possible_alpha_j_new(-2)\n",
    "        \n",
    "        # How `alpha_j` is updated depends on various conditions of `r_ij = alpha_i + alpha_j`\n",
    "        r_ij = alpha_i + alpha_j\n",
    "        \n",
    "        # Compute new alpha_j and clip it inside [L, H]. This is the update case when eta > 0\n",
    "        if r_ij == 0:\n",
    "            if possible_alpha_j_new_pos2 <= L:\n",
    "                alpha_j_new = L\n",
    "            elif L < possible_alpha_j_new_pos2 and possible_alpha_j_new_pos2 < 0:\n",
    "                alpha_j_new = possible_alpha_j_new_pos2\n",
    "            elif possible_alpha_j_new_neg2 >= H:\n",
    "                alpha_j_new = H\n",
    "            elif 0 < possible_alpha_j_new_neg2 and possible_alpha_j_new_neg2 < H:\n",
    "                alpha_j_new = possible_alpha_j_new_neg2\n",
    "            else:\n",
    "                alpha_j_new = 0\n",
    "        \n",
    "        elif 0 < r_ij and r_ij < self.C:\n",
    "            if possible_alpha_j_new_pos2 <= L:\n",
    "                alpha_j_new = L\n",
    "            elif L < possible_alpha_j_new_pos2 and possible_alpha_j_new_pos2 < 0:\n",
    "                alpha_j_new = possible_alpha_j_new_pos2\n",
    "            elif possible_alpha_j_new_zero <= 0:\n",
    "                alpha_j_new = 0 \n",
    "            elif 0 < possible_alpha_j_new_zero and possible_alpha_j_new_zero < r_ij:\n",
    "                alpha_j_new = possible_alpha_j_new_zero\n",
    "            elif possible_alpha_j_new_neg2 >= H:\n",
    "                alpha_j_new = H\n",
    "            elif r_ij < possible_alpha_j_new_neg2 and possible_alpha_j_new_neg2 < H:\n",
    "                alpha_j_new = possible_alpha_j_new_neg2\n",
    "            else:\n",
    "                alpha_j_new = r_ij\n",
    "        \n",
    "        elif r_ij == self.C:\n",
    "            if possible_alpha_j_new_zero <= L:\n",
    "                alpha_j_new = L\n",
    "            elif L < possible_alpha_j_new_zero and possible_alpha_j_new_zero < H:\n",
    "                alpha_j_new = possible_alpha_j_new_zero\n",
    "            else:\n",
    "                alpha_j_new = H\n",
    "                \n",
    "        elif r_ij > self.C:\n",
    "            if possible_alpha_j_new_zero < L:\n",
    "                alpha_j_new = L\n",
    "            elif L <= possible_alpha_j_new_zero and possible_alpha_j_new_zero <= H:\n",
    "                alpha_j_new = possible_alpha_j_new_zero\n",
    "            else:\n",
    "                alpha_j_new = H\n",
    "        \n",
    "        elif -self.C < r_ij and r_ij < 0:\n",
    "            if possible_alpha_j_new_pos2 <= L:\n",
    "                alpha_j_new = L\n",
    "            elif L < possible_alpha_j_new_pos2 and possible_alpha_j_new_pos2 < r_ij:\n",
    "                alpha_j_new = possible_alpha_j_new_pos2\n",
    "            elif possible_alpha_j_new_zero <= r_ij:\n",
    "                alpha_j_new = r_ij\n",
    "            elif r_ij < possible_alpha_j_new_zero and possible_alpha_j_new_zero < 0:\n",
    "                alpha_j_new = possible_alpha_j_new_zero\n",
    "            elif possible_alpha_j_new_neg2 >= H:\n",
    "                alpha_j_new = H\n",
    "            elif 0 < possible_alpha_j_new_neg2 and possible_alpha_j_new_neg2 < H:\n",
    "                alpha_j_new = possible_alpha_j_new_neg2\n",
    "            else:\n",
    "                alpha_j_new = 0\n",
    "        \n",
    "        elif r_ij == -self.C:\n",
    "            if possible_alpha_j_new_zero <= L:\n",
    "                alpha_j_new = L\n",
    "            elif L < possible_alpha_j_new_zero and possible_alpha_j_new_zero < H:\n",
    "                alpha_j_new = possible_alpha_j_new_zero\n",
    "            else:\n",
    "                alpha_j_new = H\n",
    "        \n",
    "        elif r_ij < -self.C:\n",
    "            if possible_alpha_j_new_zero < L:\n",
    "                alpha_j_new = L\n",
    "            elif L <= possible_alpha_j_new_zero and possible_alpha_j_new_zero <= H:\n",
    "                alpha_j_new = possible_alpha_j_new_zero\n",
    "            else:\n",
    "                alpha_j_new = H\n",
    "\n",
    "        # Compute new alpha_i based on new alpha_j\n",
    "        alpha_i_new = alpha_i - (alpha_j_new - alpha_j)\n",
    "\n",
    "        # Compute step sizes\n",
    "        delta_alpha_i = alpha_i_new - alpha_i\n",
    "        delta_alpha_j = alpha_j_new - alpha_j\n",
    "\n",
    "        # Update weight vector\n",
    "        if self.use_w:\n",
    "            self.w = self.w + delta_alpha_i*x_i + delta_alpha_j*x_j\n",
    "\n",
    "        # Update b\n",
    "        if self.enable_kernel_cache:\n",
    "            b_i = self.b - E_i - delta_alpha_i*self.kernel_cache[i][i] - delta_alpha_j*self.kernel_cache[i][j]\n",
    "            b_j = self.b - E_j - delta_alpha_i*self.kernel_cache[i][j] - delta_alpha_j*self.kernel_cache[j][j]\n",
    "        else:\n",
    "            b_i = self.b - E_i - delta_alpha_i*self.kernel(x_i, x_i) - delta_alpha_j*self.kernel(x_i, x_j)\n",
    "            b_j = self.b - E_j - delta_alpha_i*self.kernel(x_i, x_j) - delta_alpha_j*self.kernel(x_j, x_j)\n",
    "        self.b = (b_i + b_j)/2\n",
    "        if (alpha_i_new > 0 and alpha_i_new < self.C):\n",
    "            self.b = b_i\n",
    "        if (alpha_j_new > 0 and alpha_j_new < self.C):\n",
    "            self.b = b_j\n",
    "\n",
    "        # Update the alpha pair\n",
    "        self.alpha[i] = alpha_i_new\n",
    "        self.alpha[j] = alpha_j_new\n",
    "\n",
    "        return 1\n",
    "\n",
    "    def _f(self, X):\n",
    "        \"\"\" Linear classifier `f(x) = wx + b`, used when training or making predictions.\n",
    "            @param `X`: `X` can be a single example with shape(D,) or multiple examples with shape(N, D)\n",
    "        \"\"\"\n",
    "        # Use b_mean to make predictions when training is done.\n",
    "        b = self.b_mean if self.b_mean is not None else self.b\n",
    "\n",
    "        if self.use_w:\n",
    "            # Speed up by using computed weight only when linear kernel is used.\n",
    "            return np.dot(X, self.w) + b\n",
    "        else:\n",
    "            # If X is single example\n",
    "            if X.ndim == 1:\n",
    "                # ---- Not the same as SVC ----\n",
    "                return np.dot(self.alpha, self.kernel(self.train_X, X)) + b\n",
    "            # Multiple examples\n",
    "            elif X.ndim == 2:\n",
    "                return np.array([np.dot(self.alpha, self.kernel(self.train_X, _X)) + b for _X in X])\n",
    "\n",
    "    def _E(self, i):\n",
    "        \"\"\" Prediction error: _f(x_i) - y_i, used when training. \"\"\"\n",
    "        if self.enable_kernel_cache:\n",
    "            # ---- Not the same as SVC ----\n",
    "            return np.dot(self.alpha, self.kernel_cache[i]) + self.b - self.train_y[i]\n",
    "        else:\n",
    "            return self._f(self.train_X[i]) - self.train_y[i]\n",
    "\n",
    "    def _precompute_kernel_cache(self):\n",
    "        \"\"\" If self.enable_kernel_cache is True, then precompute kernel results for all training examples.\n",
    "            This can speed up training but need time to initialize when data is large.\n",
    "        \"\"\"\n",
    "        kernel_cache = np.zeros((len(self.train_X), len(self.train_X)))\n",
    "        for i, x_i in enumerate(self.train_X):\n",
    "            for j, x_j in enumerate(self.train_X):\n",
    "                kernel_cache[i][j] = self.kernel(x_i, x_j)\n",
    "        return kernel_cache\n",
    "\n",
    "    def _postcompute_biases(self):\n",
    "        \"\"\" Post-computed biases for non-boundary training examples (support vectors) when training is done.\n",
    "            This is for estimating sample mean and sample std of biases.\n",
    "            For a good learning result, sample std of biases should be small.\n",
    "        \"\"\"\n",
    "        # ---- Not the same as SVC ----\n",
    "        def _b(i):\n",
    "            if self.enable_kernel_cache:\n",
    "                return self.train_y[i] - np.dot(self.alpha, self.kernel_cache[i])\n",
    "            else:\n",
    "                return self.train_y[i] - self._f(self.train_X[i])\n",
    "\n",
    "        I_non_boundary = np.where(np.logical_and(self.alpha > 0, self.alpha < self.C) == True)[0].tolist()\n",
    "\n",
    "        if len(I_non_boundary):\n",
    "            biases = np.vectorize(_b)(I_non_boundary)\n",
    "            self.b_mean = np.mean(biases)\n",
    "            self.b_std = np.sqrt(np.sum((biases - self.b_mean)**2) / (len(biases) - 1))\n",
    "            self.postcomputed_biases[I_non_boundary] = biases\n",
    "\n",
    "    def _linear_kernel(self, X, x):\n",
    "        \"\"\" Linear kernel:\n",
    "            @param `X`: `X` can be a single example with shape(D,) or multiple examples with shape(N, D)\n",
    "            @param `x`: `x` can only be a single example with shape(D,)\n",
    "        \"\"\"\n",
    "        return np.dot(X, x)\n",
    "\n",
    "    def _poly_kernel(self, X, x):\n",
    "        \"\"\" Polynomial kernel:\n",
    "            @param `X`: `X` can be a single example with shape(D,) or multiple examples with shape(N, D)\n",
    "            @param `x`: `x` can only be a single example with shape(D,)\n",
    "        \"\"\"\n",
    "        return (1 + np.dot(X, x))**self.poly_degree\n",
    "\n",
    "    def _rbf_kernel(self, X, x):\n",
    "        \"\"\" RBF (guassian) kernel:\n",
    "            @param `X`: `X` can be a single example with shape(D,) or multiple examples with shape(N, D)\n",
    "            @param `x`: `x` can only be a single example with shape(D,)\n",
    "        \"\"\"\n",
    "        # If X is single example\n",
    "        if X.ndim == 1:\n",
    "            sqrt_norm = np.linalg.norm(X - x)**2\n",
    "        # Multiple examples\n",
    "        elif X.ndim == 2:\n",
    "            sqrt_norm = np.linalg.norm(X - x, axis=1)**2\n",
    "\n",
    "        return np.exp(-sqrt_norm / (2.0 * (self.rbf_sigma**2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Enable kernel cache. Precomputing kernel results for all training examples ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 791 alphas changed.\n",
      "[*] Iteration: 1 | Train error: 5.995280758240719 | Test error: 5.689199775154771\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 59 alphas changed.\n",
      "[*] Iteration: 100 | Train error: 5.147257681293785 | Test error: 4.849830650335117\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 34 alphas changed.\n",
      "[*] Iteration: 200 | Train error: 5.148842075222539 | Test error: 4.849980746293115\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 24 alphas changed.\n",
      "[*] Iteration: 300 | Train error: 5.14620730860149 | Test error: 4.846038532608927\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 32 alphas changed.\n",
      "[*] Iteration: 400 | Train error: 5.146869350392723 | Test error: 4.846757419426311\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 25 alphas changed.\n",
      "[*] Iteration: 500 | Train error: 5.147235819812041 | Test error: 4.846501003515553\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 39 alphas changed.\n",
      "[*] Iteration: 600 | Train error: 5.146203354873307 | Test error: 4.846028193498611\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 18 alphas changed.\n",
      "[*] Iteration: 700 | Train error: 5.146254041017459 | Test error: 4.846169663903617\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 36 alphas changed.\n",
      "[*] Iteration: 800 | Train error: 5.147077118682099 | Test error: 4.848239398806427\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 31 alphas changed.\n",
      "[*] Iteration: 900 | Train error: 5.146264151518896 | Test error: 4.846313244954814\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 25 alphas changed.\n",
      "[*] Iteration: 1000 | Train error: 5.146556196584604 | Test error: 4.848065062931198\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 42 alphas changed.\n",
      "[*] Iteration: 1100 | Train error: 5.503909110049788 | Test error: 5.137942978168429\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 29 alphas changed.\n",
      "[*] Iteration: 1200 | Train error: 5.146220822738152 | Test error: 4.846266486575069\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 24 alphas changed.\n",
      "[*] Iteration: 1300 | Train error: 5.146290208446365 | Test error: 4.8472780382437755\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 24 alphas changed.\n",
      "[*] Iteration: 1400 | Train error: 5.320193404830564 | Test error: 4.973696357270014\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 30 alphas changed.\n",
      "[*] Iteration: 1500 | Train error: 5.264436481725431 | Test error: 4.924604279851508\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 17 alphas changed.\n",
      "[*] Iteration: 1600 | Train error: 5.146270310491902 | Test error: 4.846324865928804\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 23 alphas changed.\n",
      "[*] Iteration: 1700 | Train error: 5.160229873808174 | Test error: 4.864082301267893\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 27 alphas changed.\n",
      "[*] Iteration: 1800 | Train error: 5.2820469523623705 | Test error: 4.93933546492713\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 35 alphas changed.\n",
      "[*] Iteration: 1900 | Train error: 5.169701304874084 | Test error: 4.856122407021332\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 32 alphas changed.\n",
      "[*] Iteration: 2000 | Train error: 5.149501969899745 | Test error: 4.844848165788209\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 28 alphas changed.\n",
      "[*] Iteration: 2100 | Train error: 5.186017572525084 | Test error: 4.885108996121541\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 19 alphas changed.\n",
      "[*] Iteration: 2200 | Train error: 5.146502727030228 | Test error: 4.848606078379241\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 23 alphas changed.\n",
      "[*] Iteration: 2300 | Train error: 5.148160497332652 | Test error: 4.844754366381666\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 27 alphas changed.\n",
      "[*] Iteration: 2400 | Train error: 5.146407028603351 | Test error: 4.84714109282174\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 22 alphas changed.\n",
      "[*] Iteration: 2500 | Train error: 5.146357090656024 | Test error: 4.848602069289367\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 25 alphas changed.\n",
      "[*] Iteration: 2600 | Train error: 5.183151015273112 | Test error: 4.866584832570603\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 30 alphas changed.\n",
      "[*] Iteration: 2700 | Train error: 5.156043047356192 | Test error: 4.847731503926777\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 32 alphas changed.\n",
      "[*] Iteration: 2800 | Train error: 5.146322021945631 | Test error: 4.847960641425053\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 30 alphas changed.\n",
      "[*] Iteration: 2900 | Train error: 5.2232301152404705 | Test error: 4.893528912924546\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 32 alphas changed.\n",
      "[*] Iteration: 3000 | Train error: 5.77243338692568 | Test error: 5.38757185024855\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 25 alphas changed.\n",
      "[*] Iteration: 3100 | Train error: 5.14630522712636 | Test error: 4.846509094463099\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 22 alphas changed.\n",
      "[*] Iteration: 3200 | Train error: 5.269850357705096 | Test error: 4.929276320078404\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 45 alphas changed.\n",
      "[*] Iteration: 3300 | Train error: 5.574525013184247 | Test error: 5.280179445768028\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 24 alphas changed.\n",
      "[*] Iteration: 3400 | Train error: 5.307171102719101 | Test error: 4.998597774308226\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 28 alphas changed.\n",
      "[*] Iteration: 3500 | Train error: 5.150526753384661 | Test error: 4.844824026499088\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 35 alphas changed.\n",
      "[*] Iteration: 3600 | Train error: 5.16490865980921 | Test error: 4.868211639964305\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 22 alphas changed.\n",
      "[*] Iteration: 3700 | Train error: 5.146454354263689 | Test error: 4.848473604001903\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 37 alphas changed.\n",
      "[*] Iteration: 3800 | Train error: 5.173989727518728 | Test error: 4.875686220394389\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 25 alphas changed.\n",
      "[*] Iteration: 3900 | Train error: 5.146337979488797 | Test error: 4.846001659625347\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 23 alphas changed.\n",
      "[*] Iteration: 4000 | Train error: 5.1734587420383376 | Test error: 4.8596756689845435\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 31 alphas changed.\n",
      "[*] Iteration: 4100 | Train error: 5.177315385603995 | Test error: 4.862567864222985\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 36 alphas changed.\n",
      "[*] Iteration: 4200 | Train error: 5.146354819723739 | Test error: 4.848100314657098\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 17 alphas changed.\n",
      "[*] Iteration: 4300 | Train error: 5.1463905088105575 | Test error: 4.848375705098274\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 36 alphas changed.\n",
      "[*] Iteration: 4400 | Train error: 5.163356152520016 | Test error: 4.852473534032666\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 43 alphas changed.\n",
      "[*] Iteration: 4500 | Train error: 5.262883773037378 | Test error: 4.923364007738427\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 13 alphas changed.\n",
      "[*] Iteration: 4600 | Train error: 5.14627163814796 | Test error: 4.846258076443216\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 22 alphas changed.\n",
      "[*] Iteration: 4700 | Train error: 5.153552880714236 | Test error: 4.84640264009407\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 23 alphas changed.\n",
      "[*] Iteration: 4800 | Train error: 5.146466687936885 | Test error: 4.848661351263246\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 21 alphas changed.\n",
      "[*] Iteration: 4900 | Train error: 5.1464666879368846 | Test error: 4.848661351263245\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Train on specified parameter: {'C': 1e-07, 'kernel_type': 'linear'}\n",
      "[*] 35 alphas changed.\n",
      "[*] Iteration: 5000 | Train error: 5.146270180831821 | Test error: 4.846976863961501\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Max iteration acheived.\n",
      "[*] Averaging post-computed biases as final bias of SVM hypothesis.\n"
     ]
    }
   ],
   "source": [
    "param = {\n",
    "    'C': 1e-7,\n",
    "    'kernel_type': 'linear'\n",
    "}\n",
    "\n",
    "model = SVR(data=data, param=param,\n",
    "            enable_heuristic=False,\n",
    "            enable_kernel_cache=True,\n",
    "            max_iteration=5000,\n",
    "            verbose=True)\n",
    "\n",
    "model.train(info='Train on specified parameter: {}'.format(param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Weight vector: [ -9.76928743e-04  -1.41480802e-04  -2.27064425e-06   2.43446463e-04\n",
      "  -4.38412529e-07]\n",
      "[*] Sample mean of bias: 127.99226656783974\n",
      "[*] Sample std of bias: 0.010666209920807615\n",
      "[*] Performance: Train error: 5.146690801223312 | Test error: 4.849067759675981\n"
     ]
    }
   ],
   "source": [
    "# Compute training and test error.\n",
    "train_error = mean_absolute_epsilon_error(y_truth=data['train_y'], y_pred=model.hypothesis(X=data['train_X']))\n",
    "test_error = mean_absolute_epsilon_error(y_truth=data['test_y'], y_pred=model.hypothesis(X=data['test_X']))\n",
    "\n",
    "if model.use_w: print('[*] Weight vector: {}'.format(model.w))\n",
    "print('[*] Sample mean of bias: {}'.format(model.b_mean))\n",
    "print('[*] Sample std of bias: {}'.format(model.b_std))\n",
    "print('[*] Performance: Train error: {} | Test error: {}'.format(train_error, test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.1, 0.325, 0.55, 0.775, 1.0],)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(0.1, 1, 5).round(15).tolist(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
