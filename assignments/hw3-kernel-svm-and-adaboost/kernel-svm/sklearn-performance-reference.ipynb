{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import zero_one_loss, accuracy_score\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = load_dataset(train_file_path='./dataset/hw2_grading_iris_ver_vir_training.csv',\n",
    "                   test_file_path='./dataset/hw2_grading_iris_ver_vir_testing.csv')\n",
    "\n",
    "# data = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X = data['train_X']\n",
    "train_y = data['train_y']\n",
    "test_X = data['test_X']\n",
    "test_y = data['test_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67, 4)\n",
      "(33, 4)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "print(test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zero_one_loss(y_truth, y_pred, sample_weights=None):\n",
    "    return np.average(y_pred != y_truth, weights=sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class WeakClassifer:\n",
    "    def __init__(self, data, sample_weights=None, scorer=zero_one_loss, verbose=False):\n",
    "        self.X = data['X']\n",
    "        self.y = data['y']\n",
    "        self.sample_weights = sample_weights if sample_weights is not None else np.ones_like(self.y).astype(float) / len(self.y)\n",
    "        self.param = {'feature_index': None, 'split_point': None, 'label': None}\n",
    "        self.scorer = scorer\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        best_error = np.inf\n",
    "        binary_class_label = list(set(self.y))\n",
    "\n",
    "        # Iterate over all features\n",
    "        for feature_index in range(self.X.shape[1]):\n",
    "            if self.verbose:\n",
    "                print('-'*100)\n",
    "                print('[*] Scanning Feature: {} ...'.format(feature_index))\n",
    "\n",
    "            # Find best split (hypothesis) on data w.r.t the current feature\n",
    "            x = self.X[:, feature_index]\n",
    "            split_points = list(set(x))\n",
    "            \n",
    "            for split_point in split_points:\n",
    "                # Choost the hypothesis either classifies one side +1, the other side -1\n",
    "                # or classifies one side -1, the other size +1, which leads to lowest weighted lost.\n",
    "                hypothesis1 = np.vectorize(lambda val: binary_class_label[0] if val > split_point else binary_class_label[1])\n",
    "                hypothesis2 = np.vectorize(lambda val: binary_class_label[1] if val > split_point else binary_class_label[0])\n",
    "\n",
    "                y_pred1 = hypothesis1(x)\n",
    "                y_pred2 = hypothesis2(x)\n",
    "                \n",
    "                error1 = self.scorer(y_truth=self.y, y_pred=y_pred1, sample_weights=self.sample_weights)\n",
    "                error2 = self.scorer(y_truth=self.y, y_pred=y_pred2, sample_weights=self.sample_weights)\n",
    "\n",
    "                if error1 < error2 and error1 < best_error:\n",
    "                    best_error = error1\n",
    "                    self.param.update({'feature_index': feature_index,\n",
    "                                       'split_point': split_point,\n",
    "                                       'label': binary_class_label[0]})\n",
    "                    if self.verbose:\n",
    "                        print('-'*100)\n",
    "                        print('[*] Error: {}'.format(best_error))\n",
    "                        print('[*] Update hypothesis: {}'.format(self.param))\n",
    "\n",
    "                elif error2 < error1 and error2 < best_error:\n",
    "                    best_error = error2\n",
    "                    self.param.update({'feature_index': feature_index,\n",
    "                                       'split_point': split_point,\n",
    "                                       'label': binary_class_label[1]})\n",
    "                    if self.verbose:\n",
    "                        print('-'*100)\n",
    "                        print('[*] Error: {}'.format(best_error))\n",
    "                        print('[*] Update hypothesis: {}'.format(self.param))\n",
    "\n",
    "        if self.verbose:\n",
    "            print('-'*100)\n",
    "            print('[*] Best Error: {}'.format(best_error))\n",
    "            print('[*] Best hypothesis: {}'.format(self.param))\n",
    "            print('{}'.format(self))\n",
    "            print('-'*100)\n",
    "\n",
    "    def hypothesis(self, X):\n",
    "        x = X[:, self.param['feature_index']]\n",
    "        the_other_label = list(set(self.y) - set([self.param['label']]))[0]\n",
    "        return np.vectorize(lambda val: self.param['label'] if val > self.param['split_point'] else the_other_label)(x)\n",
    "\n",
    "    def __str__(self):\n",
    "        the_other_label = list(set(self.y) - set([self.param['label']]))[0]\n",
    "\n",
    "        description = \\\n",
    "        '[*] Label {} if value at {}-th feature is greater than {}. \\n' + \\\n",
    "        '[*] Label {} if value at {}-th feature is less or equal to {}.'\n",
    "\n",
    "        return description.format(self.param['label'],\n",
    "                                  self.param['feature_index'],\n",
    "                                  self.param['split_point'],\n",
    "                                  the_other_label,\n",
    "                                  self.param['feature_index'],\n",
    "                                  self.param['split_point'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AdaboostClassifer:\n",
    "    \"\"\" Class labels need to be {+1, -1} \"\"\"\n",
    "    def __init__(self, data, param={}, scorer=zero_one_loss, verbose=False):\n",
    "\n",
    "        self.train_X = data['train_X']\n",
    "        self.train_y = data['train_y']\n",
    "        self.test_X = data['test_X']\n",
    "        self.test_y = data['test_y']\n",
    "\n",
    "        self.base_classifer = param.get('base_classifer', WeakClassifer)\n",
    "        self.num_classifer = param.get('num_classifer', 5)\n",
    "        self.scorer = scorer\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.sample_weights = np.ones_like(self.train_y).astype(float) / len(self.train_y)\n",
    "\n",
    "        self.classifer_weights = np.ones(self.num_classifer) / self.num_classifer\n",
    "        self.classifers = []\n",
    "\n",
    "    def train(self, info=''):\n",
    "        \"\"\" Perform adaptive boosting \"\"\"\n",
    "        \n",
    "        if info: \n",
    "            print('-'*100)\n",
    "            print('[*] {}'.format(info))\n",
    "        \n",
    "        for t in range(self.num_classifer):\n",
    "            classifer = self.base_classifer(data={'X': self.train_X, 'y': self.train_y}, sample_weights=self.sample_weights)\n",
    "            classifer.train()\n",
    "\n",
    "            y_pred = classifer.hypothesis(X=self.train_X)\n",
    "            y_truth = self.train_y\n",
    "            error = self.scorer(y_truth, y_pred, sample_weights=self.sample_weights)\n",
    "\n",
    "            self.classifer_weights[t] = 0.5 * np.log((1 - error) / error)\n",
    "            normalization_factor = 2 * np.sqrt(error * (1 - error))\n",
    "            self.sample_weights = self.sample_weights * np.exp(-self.classifer_weights[t]*y_truth*y_pred) / normalization_factor\n",
    "\n",
    "            self.classifers.append(classifer)\n",
    "\n",
    "            if self.verbose:\n",
    "                print('-'*100)\n",
    "                print('[*] {}-th classifer weight: {}'.format(t+1, self.classifer_weights[t]))\n",
    "                print('[*] {}-th classifer error: {}'.format(t+1, error))\n",
    "                print('[*] {}-th classifer hypothesis: {}'.format(t+1, self.classifers[t].param))\n",
    "                print('{}'.format(self.classifers[t]))\n",
    "\n",
    "    def hypothesis(self, X):\n",
    "        y_pred = np.zeros(len(X))\n",
    "        for t in range(len(self.classifers)):\n",
    "            y_pred += self.classifer_weights[t] * self.classifers[t].hypothesis(X)\n",
    "        return np.sign(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = AdaboostClassifer(data, param={'num_classifer': 5}, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 1-th classifer weight: 1.53013539735\n",
      "[*] 1-th classifer error: 0.044776119403\n",
      "[*] 1-th classifer hypothesis: {'split_point': 4.7999999999999998, 'feature_index': 2, 'label': -1}\n",
      "[*] Label -1 if value at 2-th feature is greater than 4.8. \n",
      "[*] Label 1 if value at 2-th feature is less or equal to 4.8.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 2-th classifer weight: 1.71699360224\n",
      "[*] 2-th classifer error: 0.03125\n",
      "[*] 2-th classifer hypothesis: {'split_point': 1.6000000000000001, 'feature_index': 3, 'label': -1}\n",
      "[*] Label -1 if value at 3-th feature is greater than 1.6. \n",
      "[*] Label 1 if value at 3-th feature is less or equal to 1.6.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 3-th classifer weight: 0.744273932616\n",
      "[*] 3-th classifer error: 0.184139784946\n",
      "[*] 3-th classifer hypothesis: {'split_point': 4.9000000000000004, 'feature_index': 2, 'label': -1}\n",
      "[*] Label -1 if value at 2-th feature is greater than 4.9. \n",
      "[*] Label 1 if value at 2-th feature is less or equal to 4.9.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 4-th classifer weight: 0.450648256569\n",
      "[*] 4-th classifer error: 0.288784136413\n",
      "[*] 4-th classifer hypothesis: {'split_point': 2.8999999999999999, 'feature_index': 1, 'label': -1}\n",
      "[*] Label -1 if value at 1-th feature is greater than 2.9. \n",
      "[*] Label 1 if value at 1-th feature is less or equal to 2.9.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 5-th classifer weight: 0.50216149701\n",
      "[*] 5-th classifer error: 0.26809231862\n",
      "[*] 5-th classifer hypothesis: {'split_point': 3.0, 'feature_index': 1, 'label': 1}\n",
      "[*] Label 1 if value at 1-th feature is greater than 3.0. \n",
      "[*] Label -1 if value at 1-th feature is less or equal to 3.0.\n"
     ]
    }
   ],
   "source": [
    "clf.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014925373134328358"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_one_loss(train_y, clf.hypothesis(train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.090909090909090912"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_one_loss(test_y, clf.hypothesis(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_K_fold(X, y, K, shuffle=False):\n",
    "    train_test_folds = []\n",
    "\n",
    "    for k in range(K):\n",
    "        k_train_X = np.array([_x for i, _x in enumerate(X) if i % K != k])\n",
    "        k_train_y = np.array([_y for i, _y in enumerate(y) if i % K != k])\n",
    "        k_valid_X = np.array([_x for i, _x in enumerate(X) if i % K == k])\n",
    "        k_valid_y = np.array([_y for i, _y in enumerate(y) if i % K == k])\n",
    "        \n",
    "        train_test_folds.append({'train_X': k_train_X,\n",
    "                                 'train_y': k_train_y,\n",
    "                                 'test_X': k_valid_X,\n",
    "                                 'test_y': k_valid_y})\n",
    "    return train_test_folds\n",
    "\n",
    "def K_fold_cross_validation(data, model, param, scorer=zero_one_loss, K=5):\n",
    "    # Unpack data\n",
    "    train_X = data['train_X']\n",
    "    train_y = data['train_y']\n",
    "    test_X = data['test_X']\n",
    "    test_y = data['test_y']\n",
    "    \n",
    "    # Get K folds training, validation data.\n",
    "    train_test_folds = split_K_fold(X=train_X, y=train_y, K=K)\n",
    "\n",
    "    # Perform cross-validation.\n",
    "    cv_error = 0\n",
    "    for i, data in enumerate(train_test_folds):\n",
    "\n",
    "        _model = model(data=data, param=param)\n",
    "        _model.train()\n",
    "\n",
    "        # Compute training and test error.\n",
    "        train_error = scorer(y_truth=train_y, y_pred=_model.hypothesis(X=train_X))\n",
    "        test_error = scorer(y_truth=test_y, y_pred=_model.hypothesis(X=test_X))\n",
    "        \n",
    "        print('-'*100)\n",
    "        print('[*] {}-th fold | Parameter: {}'.format(i+1, param))\n",
    "        print('[*] {}-th fold | Train error: {} | Validation error: {}'.format(i+1, train_error, test_error))\n",
    "\n",
    "        cv_error += test_error\n",
    "\n",
    "    cv_error /= K\n",
    "\n",
    "    return cv_error\n",
    "\n",
    "class GridSearchCV:\n",
    "    def __init__(self, data, model=None, param_grid={}, scorer=zero_one_loss, num_folds=5):\n",
    "        \"\"\" Exhaustive search over specified parameter values for a model. \n",
    "            \n",
    "            @param data: Dictionary of training and test data:\n",
    "                - data['train_X']: Training data with shape(N, D), where N is the number of examples, D is data dimension.\n",
    "                - data['train_y']: Training label with shape(N,)\n",
    "                - data['test_X']: Test data with shape(N, D), where N is the number of examples, D is data dimension.\n",
    "                - data['test_y']: Test label with shape(N,)\n",
    "            \n",
    "            @param `model`: Either classifier or regression class that supports model.hypothesis(X) for evaluation.\n",
    "            @param `param_grid`: Dictionary with parameters names (string) as keys and lists of parameter settings as values.\n",
    "            @param `scorer`: Model evaluation function. (Defualt: zero_one_loss)\n",
    "            @param `num_folds`: For K-fold cross-validation. (Defualt: 5)\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.model = model\n",
    "        self.params = self._span_param_grid(param_grid)\n",
    "        self.scorer = scorer\n",
    "        self.num_folds = num_folds\n",
    "        \n",
    "        self.cv_results = []\n",
    "        self.best_param = {}\n",
    "        self.best_score = 0.0\n",
    "    \n",
    "    def train(self):\n",
    "        # Find the best hyper-parameters by K-fold cross-validation.\n",
    "        for param in self.params:\n",
    "            score = K_fold_cross_validation(data=self.data,\n",
    "                                            model=self.model,\n",
    "                                            param=param,\n",
    "                                            scorer=self.scorer,\n",
    "                                            K=self.num_folds)\n",
    "            self.cv_results.append({'param': param, 'score': score})\n",
    "        \n",
    "        self.cv_results.sort(key=lambda item: item['score'])\n",
    "        self.best_score = self.cv_results[0]['score']\n",
    "        self.best_param = self.cv_results[0]['param']\n",
    "    \n",
    "    def _span_param_grid(self, param_grid):\n",
    "        \"\"\" Perform Cartesian prodcut on `self.param_grid`\n",
    "            - Input: param_grid = {'a': [1, 2], 'b': [True, False]}\n",
    "            - Output: [{'a': 1, 'b': True}, {'a': 1, 'b': False},\n",
    "                       {'a': 2, 'b': True}, {'a': 2, 'b': False}]\n",
    "        \"\"\"\n",
    "        return list(dict(zip(param_grid, x)) for x in product(*param_grid.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 1-th fold | Parameter: {'num_classifer': 1}\n",
      "[*] 1-th fold | Train error: 0.044776119403 | Validation error: 0.121212121212\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 2-th fold | Parameter: {'num_classifer': 1}\n",
      "[*] 2-th fold | Train error: 0.044776119403 | Validation error: 0.121212121212\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 3-th fold | Parameter: {'num_classifer': 1}\n",
      "[*] 3-th fold | Train error: 0.0597014925373 | Validation error: 0.0909090909091\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 1-th fold | Parameter: {'num_classifer': 2}\n",
      "[*] 1-th fold | Train error: 0.0597014925373 | Validation error: 0.0606060606061\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 2-th fold | Parameter: {'num_classifer': 2}\n",
      "[*] 2-th fold | Train error: 0.044776119403 | Validation error: 0.121212121212\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 3-th fold | Parameter: {'num_classifer': 2}\n",
      "[*] 3-th fold | Train error: 0.0597014925373 | Validation error: 0.0606060606061\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 1-th fold | Parameter: {'num_classifer': 3}\n",
      "[*] 1-th fold | Train error: 0.0298507462687 | Validation error: 0.121212121212\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 2-th fold | Parameter: {'num_classifer': 3}\n",
      "[*] 2-th fold | Train error: 0.0298507462687 | Validation error: 0.121212121212\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 3-th fold | Parameter: {'num_classifer': 3}\n",
      "[*] 3-th fold | Train error: 0.0298507462687 | Validation error: 0.0606060606061\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Cross validation history:\n",
      " - Parameter: {'num_classifer': 2} | Cross validation error: 0.0808080808081\n",
      " - Parameter: {'num_classifer': 3} | Cross validation error: 0.10101010101\n",
      " - Parameter: {'num_classifer': 1} | Cross validation error: 0.111111111111\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Best parameter: {'num_classifer': 2}\n",
      "[*] Best cross validation error: 0.0808080808081\n",
      "[*] Start to train on full training data and evaluate on test data ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 1-th classifer weight: 1.53013539735\n",
      "[*] 1-th classifer error: 0.044776119403\n",
      "[*] 1-th classifer hypothesis: {'split_point': 4.7999999999999998, 'feature_index': 2, 'label': -1}\n",
      "[*] Label -1 if value at 2-th feature is greater than 4.8. \n",
      "[*] Label 1 if value at 2-th feature is less or equal to 4.8.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 2-th classifer weight: 1.71699360224\n",
      "[*] 2-th classifer error: 0.03125\n",
      "[*] 2-th classifer hypothesis: {'split_point': 1.6000000000000001, 'feature_index': 3, 'label': -1}\n",
      "[*] Label -1 if value at 3-th feature is greater than 1.6. \n",
      "[*] Label 1 if value at 3-th feature is less or equal to 1.6.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Datetime: 01:36:59\n",
      "[*] Performance: Train error: 0.0597014925373 | Test error: 0.0606060606061\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'num_classifer': [1,2,3]}\n",
    "\n",
    "clf = GridSearchCV(data=data, model=AdaboostClassifer, param_grid=param_grid, num_folds=3)\n",
    "clf.train()\n",
    "\n",
    "print('-'*100)\n",
    "print('[*] Cross validation history:')\n",
    "for cv_result in clf.cv_results:\n",
    "    print(' - Parameter: {} | Cross validation error: {}'.format(cv_result['param'], cv_result['score']))\n",
    "print('-'*100)\n",
    "print('[*] Best parameter: {}'.format(clf.best_param))\n",
    "print('[*] Best cross validation error: {}'.format(clf.best_score))\n",
    "print('[*] Start to train on full training data and evaluate on test data ...')\n",
    "\n",
    "# Train on full training data and evaluate on test data with the best hyper-parameters.\n",
    "best_model = AdaboostClassifer(data=data, param=clf.best_param, verbose=True)\n",
    "best_model.train()\n",
    "\n",
    "# Compute training and test error.\n",
    "train_error = zero_one_loss(y_truth=train_y, y_pred=best_model.hypothesis(X=train_X))\n",
    "test_error = zero_one_loss(y_truth=test_y, y_pred=best_model.hypothesis(X=test_X))\n",
    "\n",
    "print('-'*100)\n",
    "print('[*] Datetime: {}'.format(datetime.datetime.now().strftime('%H:%M:%S')))\n",
    "print('[*] Performance: Train error: {} | Test error: {}'.format(train_error, test_error))\n",
    "print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Author: Howard (Yu-Chun) Lo \"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class SVM:\n",
    "    def __init__(self, data, param={}, verbose=False):\n",
    "        \"\"\" Binary SVM Classifer optimized with SMO algorithm\n",
    "\n",
    "            Implementation Reference:\n",
    "                - SMO supplement provided by EE6550 course\n",
    "                - Standford CS 229, Autumn 2009 The Simplified SMO Algorithm\n",
    "                - Improvment to Platt's SMO Algorithm for SVM Classifer Design, S.S. Keerthi et al\n",
    "                - A Roadmap to SVM Sequential Minimal Optimization for Classification by Ruben Ramirez-Padron\n",
    "\n",
    "            @param data: Dictionary of training and test data:\n",
    "                - data['train_X']: Training data with shape(N, D), where N is the number of examples, D is data dimension.\n",
    "                - data['train_y']: Training label with shape(N,)\n",
    "                - data['test_X']: Test data with shape(N, D), where N is the number of examples, D is data dimension.\n",
    "                - data['test_y']: Test label with shape(N,)\n",
    "\n",
    "            @param param: Dictionary of hyper-parameters:\n",
    "                - param['C']: Parameter for penalty term. (Default: 0.1)\n",
    "                - param['tol']: Tolerance for KKT conditons. (Defualt: 1e-2)\n",
    "                - param['kernel_type']: Kernel type to be used in SVM. Acceptable kernel type: 'linear', 'poly', 'rbf'. (Default: 'linear')\n",
    "                - param['poly_degree']: Degree of the polynomial kernel function ('poly'). Ignored by all other kernels. (Default: 3)\n",
    "                - param['rbf_sigma']: Sigma term in RBF (guassian). Ignored by all other kernels. (Default: 0.5)\n",
    "                - param['enable_heuristic']: Whether use Platts heuristics to train SVM. (Defualt: False)\n",
    "                - param['enable_kernel_cache']: Whether precompute kernel results. This can speed up training but need time to initialize when data is large. (Defualt: True)\n",
    "                - param['max_iteration']: Max iteration for SMO training algorithm to avoid not converging.\n",
    "        \"\"\"\n",
    "\n",
    "        # Upack training and test data\n",
    "        self.train_X = data['train_X']\n",
    "        self.train_y = data['train_y']\n",
    "        self.test_X = data['test_X']\n",
    "        self.test_y = data['test_y']\n",
    "\n",
    "        # Unpack hyper-parameters\n",
    "        self.C = param.get('C', 0.1)\n",
    "        self.tol = param.get('tol', 1e-2)\n",
    "        self.kernel_type = param.get('kernel_type', 'linear')\n",
    "        self.poly_degree = param.get('poly_degree', 3)\n",
    "        self.rbf_sigma = param.get('rbf_sigma', 0.5)\n",
    "        self.enable_heuristic = param.get('enable_heuristic', False)\n",
    "        self.enable_kernel_cache = param.get('enable_kernel_cache', True)\n",
    "        self.max_iteration = param.get('max_iteration', 20000)\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Set kernel function\n",
    "        self.kernels = {\n",
    "            'linear': self._linear_kernel,\n",
    "            'poly': self._poly_kernel,\n",
    "            'rbf': self._rbf_kernel\n",
    "        }\n",
    "        self.kernel = self.kernels[self.kernel_type]\n",
    "\n",
    "        # Precompute kernel cache\n",
    "        if self.enable_kernel_cache:\n",
    "            print('-'*100)\n",
    "            print('[*] Enable kernel cache. Precomputing kernel results for all training examples ...')\n",
    "            self.kernel_cache = self._precompute_kernel_cache()\n",
    "\n",
    "        # Model parameters\n",
    "        self.use_w = True if self.kernel_type == 'linear' else False # If linear kernel is used, use weight to perform prediction instead of alphas.\n",
    "        self.w = np.zeros(self.train_X.shape[1]) # Weight vector: shape(D,)\n",
    "        self.b = 0.0 # Bias term: scalar\n",
    "        self.alpha = np.zeros(len(self.train_X)) # Lagrange multipliers\n",
    "\n",
    "    def train(self, info=''):\n",
    "        \"\"\" Optimize alpha with either simple SMO algorithm or simple SMO combined with Platt's heuristics\n",
    "            In each iteration, the SMO algorithm solves the Lagrangian dual problem\n",
    "            which involves only two Lagrangian multipliers.\n",
    "        \"\"\"\n",
    "        if self.enable_heuristic:\n",
    "            self._heuristic_smo(info)\n",
    "        else:\n",
    "            self._simple_smo(info)\n",
    "\n",
    "    def hypothesis(self, X):\n",
    "        \"\"\" Applying our linear classifier `f(x)` to perform binary classification.\n",
    "            If f(x) >= 0, y(i) = +1\n",
    "            Else    <  0, y(i) = -1\n",
    "\n",
    "            @param `X`: X can be a single example with shape(D,) or multiple examples with shape(N, D)\n",
    "        \"\"\"\n",
    "        return np.sign(self._f(X))\n",
    "\n",
    "    def _simple_smo(self, info=''):\n",
    "\n",
    "        num_changed_alphas = 1\n",
    "        iteration = 0\n",
    "\n",
    "        while num_changed_alphas > 0:\n",
    "            num_changed_alphas = 0\n",
    "            for i in range(len(self.train_X)):\n",
    "                if self._violate_KKT_conditions(i):\n",
    "                    j = i\n",
    "                    while(j == i): j = np.random.randint(0, len(self.train_X))\n",
    "                    num_changed_alphas += self._update_alpha_pair(i, j)\n",
    "\n",
    "            if self.verbose and num_changed_alphas == 0:\n",
    "                if info: print('[*] {}'.format(info))\n",
    "                print('[*] Converged at iteration {}.'.format(iteration+1))\n",
    "                print('-'*100)\n",
    "\n",
    "            iteration += 1\n",
    "            if self.verbose and (iteration == 1 or iteration % 100 == 0 or iteration == self.max_iteration):\n",
    "                # Compute training and testing error\n",
    "                train_error = zero_one_loss(y_truth=self.train_y, y_pred=self.hypothesis(X=self.train_X))\n",
    "                test_error = zero_one_loss(y_truth=self.test_y, y_pred=self.hypothesis(X=self.test_X))\n",
    "                print('-'*100)\n",
    "                if info: print('[*] {}'.format(info))\n",
    "                print('[*] {} alphas changed.'.format(num_changed_alphas))\n",
    "                print('[*] Iteration: {} | Train error: {} | Test error: {}'.format(iteration, train_error, test_error))\n",
    "\n",
    "            if self.verbose and iteration == self.max_iteration:\n",
    "                print('-'*100)\n",
    "                print('[*] Max iteration acheived.')\n",
    "                return\n",
    "\n",
    "    def _heuristic_smo(self, info=''):\n",
    "\n",
    "        num_changed_alphas = 0\n",
    "        examine_all = 1\n",
    "        iteration = 0\n",
    "\n",
    "        while num_changed_alphas > 0 or examine_all:\n",
    "            num_changed_alphas = 0\n",
    "            if examine_all:\n",
    "                # Repeated pass iterates over entire examples.\n",
    "                for i in range(len(self.train_X)):\n",
    "                    # alpha_i needs update, select alpha_j (!= alpha_i) to jointly optimize the alpha pair\n",
    "                    if self._violate_KKT_conditions(i):\n",
    "                        j = i\n",
    "                        while(j == i): j = np.random.randint(0, len(self.train_X))\n",
    "                        # Update alpha_i and alpha_j\n",
    "                        num_changed_alphas += self._update_alpha_pair(i, j)\n",
    "                \n",
    "                if self.verbose:\n",
    "                    print('-'*100)\n",
    "                    if info: print('[*] {}'.format(info))\n",
    "                    print('[*] One passes done.')\n",
    "\n",
    "                if self.verbose and num_changed_alphas == 0:\n",
    "                    if info: print('[*] {}'.format(info))\n",
    "                    print('[*] Converged at iteration {}.'.format(iteration+1))\n",
    "                    print('-'*100)\n",
    "                elif self.verbose:\n",
    "                    if info: print('[*] {}'.format(info))\n",
    "                    print('[*] Go to repeated passes.')\n",
    "            else:\n",
    "                # Repeated pass iterates over non-boundary examples.\n",
    "                I_non_boundary = np.where(np.logical_and(self.alpha > 0, self.alpha < self.C) == True)[0].tolist()\n",
    "                if len(I_non_boundary):\n",
    "                    E_list = np.vectorize(self._E)(I_non_boundary)\n",
    "                    if not max(E_list) - min(E_list) < 1:\n",
    "                        for i in I_non_boundary:\n",
    "                            num_changed_alphas += self._examine_example(i)\n",
    "\n",
    "                if self.verbose and num_changed_alphas == 0:\n",
    "                    print('-'*100)\n",
    "                    if info: print('[*] {}'.format(info))\n",
    "                    print('[*] Repeated passes done. Go back to one pass.')\n",
    "\n",
    "            if examine_all == 1:\n",
    "                # One pass done, go to repeated passes.\n",
    "                examine_all = 0\n",
    "            elif num_changed_alphas == 0:\n",
    "                # Repeated pass done, go back to one pass.\n",
    "                examine_all = 1\n",
    "\n",
    "            iteration += 1\n",
    "            if self.verbose and (iteration == 1 or iteration % 100 == 0 or iteration == self.max_iteration):\n",
    "                # Compute training and testing error\n",
    "                train_error = zero_one_loss(y_truth=self.train_y, y_pred=self.hypothesis(X=self.train_X))\n",
    "                test_error = zero_one_loss(y_truth=self.test_y, y_pred=self.hypothesis(X=self.test_X))\n",
    "                print('-'*100)\n",
    "                if info: print('[*] {}'.format(info))\n",
    "                print('[*] {} alphas changed.'.format(num_changed_alphas))\n",
    "                print('[*] Iteration: {} | Train error: {} | Test error: {}'.format(iteration, train_error, test_error))\n",
    "\n",
    "            if self.verbose and iteration == self.max_iteration:\n",
    "                print('-'*100)\n",
    "                print('[*] Max iteration acheived.')\n",
    "                return\n",
    "\n",
    "    def _violate_KKT_conditions(self, i):\n",
    "        \"\"\" Check if an example violates the KKT conditons \"\"\"\n",
    "        alpha_i = self.alpha[i]\n",
    "        R_i = self.train_y[i]*self._E(i)\n",
    "        return (R_i < -self.tol and alpha_i < self.C) or (R_i > self.tol and alpha_i > 0)\n",
    "\n",
    "    def _examine_example(self, i):\n",
    "        \"\"\" Implement Platt's heuristics to select a good alpha pair to optimize.\n",
    "            (First heuristic is not implemented since it makes training slower)\n",
    "        \"\"\"\n",
    "        # Check if alpha_i needs updating (alpha_i violates KKT conditions)\n",
    "        if self._violate_KKT_conditions(i):\n",
    "\n",
    "            # Retrieve indexes of non boundary examples\n",
    "            I_non_boundary = np.where(np.logical_and(self.alpha > 0, self.alpha < self.C) == True)[0].tolist()\n",
    "\n",
    "            # Iterate over non-boundary items, starting at a random position\n",
    "            shuffled_I_non_boundary = np.copy(I_non_boundary)\n",
    "            np.random.shuffle(shuffled_I_non_boundary)\n",
    "            for j in shuffled_I_non_boundary:\n",
    "                if self._update_alpha_pair(i, j):\n",
    "                    return 1\n",
    "\n",
    "            # Iterate over entire items, starting at a random position\n",
    "            I = np.arange(len(self.train_X))\n",
    "            shuffled_I = np.copy(I)\n",
    "            np.random.shuffle(shuffled_I)\n",
    "            for j in shuffled_I:\n",
    "                if self._update_alpha_pair(i, j):\n",
    "                    return 1\n",
    "        return 0\n",
    "\n",
    "    def _update_alpha_pair(self, i, j):\n",
    "        \"\"\" Jointly optimized alpha_i and alpha_j \"\"\"\n",
    "        # Not the alpha pair.\n",
    "        if i == j: return 0\n",
    "\n",
    "        E_i = self._E(i)\n",
    "        E_j = self._E(j)\n",
    "\n",
    "        alpha_i = self.alpha[i]\n",
    "        alpha_j = self.alpha[j]\n",
    "\n",
    "        x_i, x_j, y_i, y_j = self.train_X[i], self.train_X[j], self.train_y[i], self.train_y[j]\n",
    "\n",
    "        if y_i == y_j:\n",
    "            L = max(0, alpha_i + alpha_j - self.C)\n",
    "            H = min(self.C, alpha_i + alpha_j)\n",
    "        else:\n",
    "            L = max(0, alpha_j - alpha_i)\n",
    "            H = min(self.C, self.C + alpha_j - alpha_i)\n",
    "\n",
    "        # This will not make any progress.\n",
    "        if L == H: return 0\n",
    "\n",
    "        # Compute eta (second derivative of the Lagrange dual function = -eta)\n",
    "        if self.enable_kernel_cache:\n",
    "            eta = self.kernel_cache[i][i] + self.kernel_cache[j][j] - 2*self.kernel_cache[i][j]\n",
    "        else:\n",
    "            eta = self.kernel(x_i, x_i) + self.kernel(x_j, x_j) - 2*self.kernel(x_i, x_j)\n",
    "\n",
    "        # eta > 0 => second derivative(-eta) < 0 => maximum exists.\n",
    "        if eta <= 0: return 0\n",
    "\n",
    "        # Compute new alpha_j and clip it inside [L, H]\n",
    "        alpha_j_new = alpha_j + y_j*(E_i - E_j)/eta\n",
    "        if alpha_j_new < L: alpha_j_new = L\n",
    "        if alpha_j_new > H: alpha_j_new = H\n",
    "\n",
    "        # Compute new alpha_i based on new alpha_j\n",
    "        alpha_i_new = alpha_i + y_i*y_j*(alpha_j - alpha_j_new)\n",
    "\n",
    "        # Compute step sizes\n",
    "        delta_alpha_i = alpha_i_new - alpha_i\n",
    "        delta_alpha_j = alpha_j_new - alpha_j\n",
    "\n",
    "        # Update weight vector\n",
    "        if self.use_w:\n",
    "            self.w = self.w + delta_alpha_i*y_i*x_i + delta_alpha_j*y_j*x_j\n",
    "\n",
    "        # Update b\n",
    "        if self.enable_kernel_cache:\n",
    "            b_i = self.b - E_i - delta_alpha_i*y_i*self.kernel_cache[i][i] - delta_alpha_j*y_j*self.kernel_cache[i][j]\n",
    "            b_j = self.b - E_j - delta_alpha_i*y_i*self.kernel_cache[i][j] - delta_alpha_j*y_j*self.kernel_cache[j][j]\n",
    "        else:\n",
    "            b_i = self.b - E_i - delta_alpha_i*y_i*self.kernel(x_i, x_i) - delta_alpha_j*y_j*self.kernel(x_i, x_j)\n",
    "            b_j = self.b - E_j - delta_alpha_i*y_i*self.kernel(x_i, x_j) - delta_alpha_j*y_j*self.kernel(x_j, x_j)\n",
    "        self.b = (b_i + b_j)/2\n",
    "        if (alpha_i_new > 0 and alpha_i_new < self.C):\n",
    "            self.b = b_i\n",
    "        if (alpha_j_new > 0 and alpha_j_new < self.C):\n",
    "            self.b = b_j\n",
    "\n",
    "        # Update the alpha pair\n",
    "        self.alpha[i] = alpha_i_new\n",
    "        self.alpha[j] = alpha_j_new\n",
    "\n",
    "        return 1\n",
    "\n",
    "    def _f(self, X):\n",
    "        \"\"\" Linear classifier `f(x)`, used when training or making predictions.\n",
    "            @param `X`: `X` can be a single example with shape(D,) or multiple examples with shape(N, D)\n",
    "        \"\"\"\n",
    "        if self.use_w:\n",
    "            # Speed up by using computed weight only when linear kernel is used.\n",
    "            return np.dot(X, self.w) + self.b\n",
    "        else:\n",
    "            # If X is single example\n",
    "            if X.ndim == 1:\n",
    "                return np.dot(self.alpha*self.train_y, self.kernel(self.train_X, X)) + self.b\n",
    "            # Multiple examples\n",
    "            elif X.ndim == 2:\n",
    "                return np.array([np.dot(self.alpha*self.train_y, self.kernel(self.train_X, _X)) + self.b for _X in X])\n",
    "\n",
    "    def _E(self, i):\n",
    "        \"\"\" Prediction error: _f(x_i) - y_i, used when training. \"\"\"\n",
    "        if self.enable_kernel_cache:\n",
    "            return np.dot(self.alpha*self.train_y, self.kernel_cache[i]) + self.b - self.train_y[i]\n",
    "        else:\n",
    "            return self._f(self.train_X[i]) - self.train_y[i]\n",
    "\n",
    "    def _precompute_kernel_cache(self):\n",
    "        \"\"\" If self.enable_kernel_cache is True, then precompute kernel results for all training examples.\n",
    "            This can speed up training but need time to initialize when data is large.\n",
    "        \"\"\"\n",
    "        kernel_cache = np.zeros((len(self.train_X), len(self.train_X)))\n",
    "        for i, x_i in enumerate(self.train_X):\n",
    "            for j, x_j in enumerate(self.train_X):\n",
    "                kernel_cache[i][j] = self.kernel(x_i, x_j)\n",
    "        return kernel_cache\n",
    "\n",
    "    def _linear_kernel(self, X, x):\n",
    "        \"\"\" Linear kernel:\n",
    "            @param `X`: `X` can be a single example with shape(D,) or multiple examples with shape(N, D)\n",
    "            @param `x`: `x` can only be a single example with shape(D,)\n",
    "        \"\"\"\n",
    "        return np.dot(X, x)\n",
    "\n",
    "    def _poly_kernel(self, X, x):\n",
    "        \"\"\" Polynomial kernel:\n",
    "            @param `X`: `X` can be a single example with shape(D,) or multiple examples with shape(N, D)\n",
    "            @param `x`: `x` can only be a single example with shape(D,)\n",
    "        \"\"\"\n",
    "        return (1 + np.dot(X, x))**self.poly_degree\n",
    "\n",
    "    def _rbf_kernel(self, X, x):\n",
    "        \"\"\" RBF (guassian) kernel:\n",
    "            @param `X`: `X` can be a single example with shape(D,) or multiple examples with shape(N, D)\n",
    "            @param `x`: `x` can only be a single example with shape(D,)\n",
    "        \"\"\"\n",
    "        # If X is single example\n",
    "        if X.ndim == 1:\n",
    "            sqrt_norm = np.linalg.norm(X - x)**2\n",
    "        # Multiple examples\n",
    "        elif X.ndim == 2:\n",
    "            sqrt_norm = np.linalg.norm(X - x, axis=1)**2\n",
    "\n",
    "        return np.exp(-sqrt_norm / (2.0 * (self.rbf_sigma**2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Enable kernel cache. Precomputing kernel results for all training examples ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 1-th fold | Parameter: {'kernel_type': 'poly', 'C': 0.1, 'poly_degree': 1}\n",
      "[*] 1-th fold | Train error: 0.044776119403 | Validation error: 0.0909090909091\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Enable kernel cache. Precomputing kernel results for all training examples ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 2-th fold | Parameter: {'kernel_type': 'poly', 'C': 0.1, 'poly_degree': 1}\n",
      "[*] 2-th fold | Train error: 0.0298507462687 | Validation error: 0.0909090909091\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Enable kernel cache. Precomputing kernel results for all training examples ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 3-th fold | Parameter: {'kernel_type': 'poly', 'C': 0.1, 'poly_degree': 1}\n",
      "[*] 3-th fold | Train error: 0.044776119403 | Validation error: 0.0606060606061\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Enable kernel cache. Precomputing kernel results for all training examples ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 1-th fold | Parameter: {'kernel_type': 'poly', 'C': 0.1, 'poly_degree': 2}\n",
      "[*] 1-th fold | Train error: 0.0298507462687 | Validation error: 0.121212121212\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Enable kernel cache. Precomputing kernel results for all training examples ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 2-th fold | Parameter: {'kernel_type': 'poly', 'C': 0.1, 'poly_degree': 2}\n",
      "[*] 2-th fold | Train error: 0.044776119403 | Validation error: 0.0606060606061\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Enable kernel cache. Precomputing kernel results for all training examples ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 3-th fold | Parameter: {'kernel_type': 'poly', 'C': 0.1, 'poly_degree': 2}\n",
      "[*] 3-th fold | Train error: 0.0298507462687 | Validation error: 0.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Enable kernel cache. Precomputing kernel results for all training examples ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 1-th fold | Parameter: {'kernel_type': 'poly', 'C': 0.1, 'poly_degree': 3}\n",
      "[*] 1-th fold | Train error: 0.0149253731343 | Validation error: 0.121212121212\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Enable kernel cache. Precomputing kernel results for all training examples ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 2-th fold | Parameter: {'kernel_type': 'poly', 'C': 0.1, 'poly_degree': 3}\n",
      "[*] 2-th fold | Train error: 0.0298507462687 | Validation error: 0.0909090909091\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Enable kernel cache. Precomputing kernel results for all training examples ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 3-th fold | Parameter: {'kernel_type': 'poly', 'C': 0.1, 'poly_degree': 3}\n",
      "[*] 3-th fold | Train error: 0.044776119403 | Validation error: 0.0606060606061\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Enable kernel cache. Precomputing kernel results for all training examples ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 1-th fold | Parameter: {'kernel_type': 'poly', 'C': 0.2, 'poly_degree': 1}\n",
      "[*] 1-th fold | Train error: 0.0597014925373 | Validation error: 0.121212121212\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Enable kernel cache. Precomputing kernel results for all training examples ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 2-th fold | Parameter: {'kernel_type': 'poly', 'C': 0.2, 'poly_degree': 1}\n",
      "[*] 2-th fold | Train error: 0.0298507462687 | Validation error: 0.0909090909091\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Enable kernel cache. Precomputing kernel results for all training examples ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 3-th fold | Parameter: {'kernel_type': 'poly', 'C': 0.2, 'poly_degree': 1}\n",
      "[*] 3-th fold | Train error: 0.0298507462687 | Validation error: 0.0606060606061\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Enable kernel cache. Precomputing kernel results for all training examples ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 1-th fold | Parameter: {'kernel_type': 'poly', 'C': 0.2, 'poly_degree': 2}\n",
      "[*] 1-th fold | Train error: 0.0298507462687 | Validation error: 0.121212121212\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Enable kernel cache. Precomputing kernel results for all training examples ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 2-th fold | Parameter: {'kernel_type': 'poly', 'C': 0.2, 'poly_degree': 2}\n",
      "[*] 2-th fold | Train error: 0.044776119403 | Validation error: 0.0606060606061\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Enable kernel cache. Precomputing kernel results for all training examples ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 3-th fold | Parameter: {'kernel_type': 'poly', 'C': 0.2, 'poly_degree': 2}\n",
      "[*] 3-th fold | Train error: 0.044776119403 | Validation error: 0.0606060606061\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Enable kernel cache. Precomputing kernel results for all training examples ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 1-th fold | Parameter: {'kernel_type': 'poly', 'C': 0.2, 'poly_degree': 3}\n",
      "[*] 1-th fold | Train error: 0.0149253731343 | Validation error: 0.121212121212\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Enable kernel cache. Precomputing kernel results for all training examples ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 2-th fold | Parameter: {'kernel_type': 'poly', 'C': 0.2, 'poly_degree': 3}\n",
      "[*] 2-th fold | Train error: 0.0298507462687 | Validation error: 0.0909090909091\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Enable kernel cache. Precomputing kernel results for all training examples ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 3-th fold | Parameter: {'kernel_type': 'poly', 'C': 0.2, 'poly_degree': 3}\n",
      "[*] 3-th fold | Train error: 0.044776119403 | Validation error: 0.0606060606061\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Enable kernel cache. Precomputing kernel results for all training examples ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 1-th fold | Parameter: {'kernel_type': 'poly', 'C': 0.3, 'poly_degree': 1}\n",
      "[*] 1-th fold | Train error: 0.0597014925373 | Validation error: 0.0909090909091\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Enable kernel cache. Precomputing kernel results for all training examples ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 2-th fold | Parameter: {'kernel_type': 'poly', 'C': 0.3, 'poly_degree': 1}\n",
      "[*] 2-th fold | Train error: 0.0298507462687 | Validation error: 0.0606060606061\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Enable kernel cache. Precomputing kernel results for all training examples ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 3-th fold | Parameter: {'kernel_type': 'poly', 'C': 0.3, 'poly_degree': 1}\n",
      "[*] 3-th fold | Train error: 0.0149253731343 | Validation error: 0.0606060606061\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Enable kernel cache. Precomputing kernel results for all training examples ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 1-th fold | Parameter: {'kernel_type': 'poly', 'C': 0.3, 'poly_degree': 2}\n",
      "[*] 1-th fold | Train error: 0.0149253731343 | Validation error: 0.121212121212\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Enable kernel cache. Precomputing kernel results for all training examples ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 2-th fold | Parameter: {'kernel_type': 'poly', 'C': 0.3, 'poly_degree': 2}\n",
      "[*] 2-th fold | Train error: 0.044776119403 | Validation error: 0.0606060606061\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Enable kernel cache. Precomputing kernel results for all training examples ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 3-th fold | Parameter: {'kernel_type': 'poly', 'C': 0.3, 'poly_degree': 2}\n",
      "[*] 3-th fold | Train error: 0.044776119403 | Validation error: 0.0909090909091\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Enable kernel cache. Precomputing kernel results for all training examples ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 1-th fold | Parameter: {'kernel_type': 'poly', 'C': 0.3, 'poly_degree': 3}\n",
      "[*] 1-th fold | Train error: 0.0149253731343 | Validation error: 0.121212121212\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Enable kernel cache. Precomputing kernel results for all training examples ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 2-th fold | Parameter: {'kernel_type': 'poly', 'C': 0.3, 'poly_degree': 3}\n",
      "[*] 2-th fold | Train error: 0.0298507462687 | Validation error: 0.0909090909091\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Enable kernel cache. Precomputing kernel results for all training examples ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] 3-th fold | Parameter: {'kernel_type': 'poly', 'C': 0.3, 'poly_degree': 3}\n",
      "[*] 3-th fold | Train error: 0.044776119403 | Validation error: 0.0606060606061\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Cross validation history:\n",
      " - Parameter: {'kernel_type': 'poly', 'C': 0.1, 'poly_degree': 2} | Cross validation error: 0.0606060606061\n",
      " - Parameter: {'kernel_type': 'poly', 'C': 0.3, 'poly_degree': 1} | Cross validation error: 0.0707070707071\n",
      " - Parameter: {'kernel_type': 'poly', 'C': 0.1, 'poly_degree': 1} | Cross validation error: 0.0808080808081\n",
      " - Parameter: {'kernel_type': 'poly', 'C': 0.2, 'poly_degree': 2} | Cross validation error: 0.0808080808081\n",
      " - Parameter: {'kernel_type': 'poly', 'C': 0.1, 'poly_degree': 3} | Cross validation error: 0.0909090909091\n",
      " - Parameter: {'kernel_type': 'poly', 'C': 0.2, 'poly_degree': 1} | Cross validation error: 0.0909090909091\n",
      " - Parameter: {'kernel_type': 'poly', 'C': 0.2, 'poly_degree': 3} | Cross validation error: 0.0909090909091\n",
      " - Parameter: {'kernel_type': 'poly', 'C': 0.3, 'poly_degree': 2} | Cross validation error: 0.0909090909091\n",
      " - Parameter: {'kernel_type': 'poly', 'C': 0.3, 'poly_degree': 3} | Cross validation error: 0.0909090909091\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Best parameter: {'kernel_type': 'poly', 'C': 0.1, 'poly_degree': 2}\n",
      "[*] Best cross validation error: 0.0606060606061\n",
      "[*] Start to train on full training data and evaluate on test data ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Enable kernel cache. Precomputing kernel results for all training examples ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Datetime: 01:38:17\n",
      "[*] Performance: Train error: 0.0298507462687 | Test error: 0.0606060606061\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1, 0.2, 0.3], \n",
    "    'kernel_type': ['poly'], \n",
    "    'poly_degree': [1, 2, 3]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(data=data, model=SVM, param_grid=param_grid, num_folds=3)\n",
    "clf.train()\n",
    "\n",
    "print('-'*100)\n",
    "print('[*] Cross validation history:')\n",
    "for cv_result in clf.cv_results:\n",
    "    print(' - Parameter: {} | Cross validation error: {}'.format(cv_result['param'], cv_result['score']))\n",
    "print('-'*100)\n",
    "print('[*] Best parameter: {}'.format(clf.best_param))\n",
    "print('[*] Best cross validation error: {}'.format(clf.best_score))\n",
    "print('[*] Start to train on full training data and evaluate on test data ...')\n",
    "\n",
    "# Train on full training data and evaluate on test data with the best hyper-parameters.\n",
    "best_model = SVM(data=data, param=clf.best_param)\n",
    "best_model.train()\n",
    "\n",
    "# Compute training and test error.\n",
    "train_error = zero_one_loss(y_truth=train_y, y_pred=best_model.hypothesis(X=train_X))\n",
    "test_error = zero_one_loss(y_truth=test_y, y_pred=best_model.hypothesis(X=test_X))\n",
    "\n",
    "print('-'*100)\n",
    "print('[*] Datetime: {}'.format(datetime.datetime.now().strftime('%H:%M:%S')))\n",
    "print('[*] Performance: Train error: {} | Test error: {}'.format(train_error, test_error))\n",
    "print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.1,\n",
       "  0.16,\n",
       "  0.23,\n",
       "  0.29,\n",
       "  0.36,\n",
       "  0.42,\n",
       "  0.49,\n",
       "  0.55,\n",
       "  0.61,\n",
       "  0.68,\n",
       "  0.74,\n",
       "  0.81,\n",
       "  0.87,\n",
       "  0.94,\n",
       "  1.0],)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(0.1, 1.0, 15).round(2).tolist(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "I_non_boundary = np.where(np.logical_and(best_model.alpha > 0, best_model.alpha < best_model.C) == True)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.64890195662\n",
      "7.65202804117\n",
      "7.63977502141\n",
      "7.64890195662\n"
     ]
    }
   ],
   "source": [
    "for i in I_non_boundary:\n",
    "    print(train_y[i] - np.dot(best_model.alpha*best_model.train_y, best_model.kernel_cache[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.6489019566223559"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
