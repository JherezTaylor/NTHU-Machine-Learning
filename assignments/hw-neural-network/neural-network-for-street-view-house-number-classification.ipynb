{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sio.loadmat('SVHN.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = data['train_x'].astype('float')\n",
    "y_train = data['train_label'].astype('float')\n",
    "X_test = data['test_x'].astype('float')\n",
    "y_test = data['test_label'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Utils \"\"\"\n",
    "def onehot_to_label(onehot):\n",
    "    return np.argwhere(onehot == 1)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Cost functions \"\"\"\n",
    "\n",
    "\"\"\" Reference \n",
    "- http://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/\n",
    "- http://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "\"\"\"\n",
    "# This is actually an activation function.\n",
    "def softmax(z):\n",
    "    cache = z\n",
    "#     The following code is not stable enough (`exp(large number)` leads to `inf`)\n",
    "#     a = np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "    shifts = z - np.max(z, axis=1, keepdims=True)\n",
    "    exps = np.exp(shifts)\n",
    "    probs = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "    return probs, cache\n",
    "\n",
    "def softmax_cross_entropy_loss(z, y):\n",
    "    y_pred, _ = softmax(z)\n",
    "    N = z.shape[0]\n",
    "#     empirical error\n",
    "    loss = -np.sum(y * np.log(y_pred)) / N \n",
    "#     partial deriative of loss w.r.t y_pred\n",
    "    d_loss = (y_pred - y) / N\n",
    "    return loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Activations \"\"\"\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1 - sigmoid(z))\n",
    "\n",
    "def affine_forward(x, W, b):\n",
    "    z = np.dot(x, W) + b\n",
    "    cache = (x, W, b)\n",
    "    return z, cache\n",
    "\n",
    "def affine_backward(dout, cache):\n",
    "    x, W, b = cache\n",
    "    dx = np.dot(dout, W.T).reshape(x.shape)\n",
    "    dW = np.dot(x.T, dout)\n",
    "    db = np.sum(dout, axis=0)\n",
    "    return dx, dW, db\n",
    "\n",
    "def relu_forward(z):\n",
    "    a = np.maximum(0, z)\n",
    "    cache = z\n",
    "    return a, cache\n",
    "\n",
    "def relu_backward(dout, cache):\n",
    "    z = cache\n",
    "    da = np.array(dout, copy=True)\n",
    "    da[z <= 0] = 0\n",
    "    return da\n",
    "\n",
    "def affine_relu_forward(x, W, b):\n",
    "    z, affine_cache = affine_forward(x, W, b)\n",
    "    a, relu_cache = relu_forward(z)\n",
    "    cache = (affine_cache, relu_cache)\n",
    "    return a, cache\n",
    "\n",
    "def relu_affine_backward(dout, cache):\n",
    "    affine_cache, relu_cache = cache\n",
    "    da = relu_backward(dout, relu_cache)\n",
    "    dx, dW, db = affine_backward(da, affine_cache)\n",
    "    return dx, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TwoLayerNet(object):\n",
    "    \"\"\" Two-layer neural network \"\"\"\n",
    "    \n",
    "    def __init__(self, data, num_epoch=100, batch_size=100, input_dim=784, hidden_dim=100, output_dim=10, \n",
    "                 learning_rate=0.001, reg_lambda=0.05):\n",
    "        # Unpack data\n",
    "        self.X_train = data['X_train']\n",
    "        self.y_train = data['y_train']\n",
    "        self.X_test = data['X_test']\n",
    "        self.y_test = data['y_test']\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.num_epoch = num_epoch\n",
    "        self.batch_size = batch_size # full batch: train_X.shape[0]\n",
    "        self.input_dim = input_dim # train_X.shape[1]\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim # train_y.shape[1]\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_lambda = reg_lambda\n",
    "\n",
    "        # Model parameters\n",
    "        self.params = {}\n",
    "        W1 = np.random.randn(input_dim, hidden_dim) / np.sqrt(batch_size)\n",
    "        b1 = np.zeros(hidden_dim)\n",
    "        W2 = np.random.randn(hidden_dim, output_dim) / np.sqrt(hidden_dim)\n",
    "        b2 = np.zeros(output_dim)\n",
    "        self.params.update({ 'W1': W1, 'W2': W2, 'b1': b1, 'b2': b2 })\n",
    "        \n",
    "    def update_parameters(self, grads):\n",
    "        \"\"\" Perform parameter update \"\"\"\n",
    "        for params_name, params_val in self.params.items():\n",
    "            params_grads = grads[params_name]\n",
    "            params_val -= self.learning_rate * params_grads # Gradient descent update rule\n",
    "            self.params[params_name] = params_val\n",
    "    \n",
    "    def check_accuracy(self, X, y):\n",
    "        \"\"\" Unpack model parameters \"\"\"\n",
    "        W1, b1, W2, b2 = self.params['W1'], self.params['b1'], self.params['W2'], self.params['b2']\n",
    "        \n",
    "        \"\"\" Forward \"\"\"\n",
    "        z1 = X.dot(W1) + b1\n",
    "        a1 = sigmoid(z1)\n",
    "        z2 = a1.dot(W2) + b2\n",
    "        exp_scores = np.exp(z2)\n",
    "        y_pred = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "        \"\"\" Make predictions (no need to compute loss) \"\"\"\n",
    "        label_pred = np.argmax(y_pred, axis=1)\n",
    "        label_y = onehot_to_label(y)\n",
    "        acc = np.mean(label_pred == label_y)\n",
    "        \n",
    "        return acc\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        train_size = self.X_train.shape[0]\n",
    "        num_iteration_per_epoch = train_size // self.batch_size\n",
    "        \n",
    "        \"\"\" Start training \"\"\"\n",
    "        for epoch in range(self.num_epoch):\n",
    "            \n",
    "            for iteration in range(num_iteration_per_epoch):\n",
    "                \"\"\" Make a minibatch of training data randomly \"\"\"\n",
    "                batch_indexs = np.random.choice(train_size, self.batch_size)\n",
    "                X_batch = self.X_train[batch_indexs]\n",
    "                y_batch = self.y_train[batch_indexs]\n",
    "                \n",
    "                \"\"\" Unpack parameters from model \"\"\"\n",
    "                W1, b1, W2, b2 = self.params['W1'], self.params['b1'], self.params['W2'], self.params['b2']\n",
    "                \n",
    "                \"\"\" Forward \"\"\"\n",
    "                z1 = X_batch.dot(W1) + b1\n",
    "                a1 = sigmoid(z1)\n",
    "                z2 = a1.dot(W2) + b2\n",
    "                exp_scores = np.exp(z2)\n",
    "                y_pred = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "                \n",
    "                \"\"\" Compute loss \"\"\"\n",
    "                loss = -np.sum(y_batch * np.log(y_pred)) / self.batch_size\n",
    "                \n",
    "                \"\"\" Backward \"\"\"\n",
    "                # http://neuralnetworksanddeeplearning.com/chap2.html\n",
    "                grads = {}\n",
    "                # output layer\n",
    "                delta_output = (y_pred - y_batch)\n",
    "                dW2 = (a1.T).dot(delta_output)\n",
    "                db2 = np.sum(delta_output, axis=0)\n",
    "                # hidden layer\n",
    "                delta_hidden = delta_output.dot(W2.T) * sigmoid_prime(z1)\n",
    "                dW1 = np.dot(X_batch.T, delta_hidden)\n",
    "                db1 = np.sum(delta_hidden, axis=0)\n",
    "                \n",
    "                # Add L2 regularization term for weight gradents (biases don't need reg. terms)\n",
    "                dW2 += self.reg_lambda * W2\n",
    "                dW1 += self.reg_lambda * W1\n",
    "                grads.update({ 'W1': dW1, 'W2': dW2, 'b1': db1, 'b2': db2 })\n",
    "                \n",
    "                \"\"\" Update parameters by gradient descent \"\"\"\n",
    "                self.update_parameters(grads)\n",
    "\n",
    "                \"\"\" Print loss for every minibatch and last iteration \"\"\"\n",
    "                if iteration % self.batch_size == 0 or iteration == num_iteration_per_epoch - 1:\n",
    "                    print('Epoch: {}/{}, Iteration: {}/{}, Loss: {}'.format(epoch+1, self.num_epoch, iteration+1, num_iteration_per_epoch, loss))\n",
    "                    \n",
    "            \"\"\" Evaluate accuracy at the end of every epoch\"\"\"\n",
    "            train_acc = self.check_accuracy(self.X_train, self.y_train)\n",
    "            test_acc = self.check_accuracy(self.X_test, self.y_test)\n",
    "            print('-'*100)\n",
    "            print('Epoch: {}/{}, Train accuracy: {}, Test accuracy: {}'.format(epoch+1, self.num_epoch, train_acc, test_acc))\n",
    "            print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, Iteration: 1/450, Loss: 2.427123108381142\n",
      "Epoch: 1/100, Iteration: 101/450, Loss: 2.2998353232156026\n",
      "Epoch: 1/100, Iteration: 201/450, Loss: 2.3195042562862223\n",
      "Epoch: 1/100, Iteration: 301/450, Loss: 2.3073316113820126\n",
      "Epoch: 1/100, Iteration: 401/450, Loss: 2.2922773268783825\n",
      "Epoch: 1/100, Iteration: 450/450, Loss: 2.298665572551617\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 1/100, Train accuracy: 0.10095555555555556, Test accuracy: 0.10233333333333333\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 2/100, Iteration: 1/450, Loss: 2.3151929275528262\n",
      "Epoch: 2/100, Iteration: 101/450, Loss: 2.3278845691565873\n",
      "Epoch: 2/100, Iteration: 201/450, Loss: 2.2993400128319403\n",
      "Epoch: 2/100, Iteration: 301/450, Loss: 2.2911113382094648\n",
      "Epoch: 2/100, Iteration: 401/450, Loss: 2.2757056506132813\n",
      "Epoch: 2/100, Iteration: 450/450, Loss: 2.3118204379846055\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 2/100, Train accuracy: 0.12066666666666667, Test accuracy: 0.1184\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 3/100, Iteration: 1/450, Loss: 2.306936173488795\n",
      "Epoch: 3/100, Iteration: 101/450, Loss: 2.2673556431372157\n",
      "Epoch: 3/100, Iteration: 201/450, Loss: 2.2890659036254575\n",
      "Epoch: 3/100, Iteration: 301/450, Loss: 2.279379819912342\n",
      "Epoch: 3/100, Iteration: 401/450, Loss: 2.2779339047838176\n",
      "Epoch: 3/100, Iteration: 450/450, Loss: 2.326567198040831\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 3/100, Train accuracy: 0.15646666666666667, Test accuracy: 0.14473333333333332\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 4/100, Iteration: 1/450, Loss: 2.274202090386958\n",
      "Epoch: 4/100, Iteration: 101/450, Loss: 2.275222892086423\n",
      "Epoch: 4/100, Iteration: 201/450, Loss: 2.316910588923731\n",
      "Epoch: 4/100, Iteration: 301/450, Loss: 2.2663601608741324\n",
      "Epoch: 4/100, Iteration: 401/450, Loss: 2.2745878875660526\n",
      "Epoch: 4/100, Iteration: 450/450, Loss: 2.2602510381197582\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 4/100, Train accuracy: 0.15355555555555556, Test accuracy: 0.14653333333333332\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 5/100, Iteration: 1/450, Loss: 2.269658895933931\n",
      "Epoch: 5/100, Iteration: 101/450, Loss: 2.2487634092657816\n",
      "Epoch: 5/100, Iteration: 201/450, Loss: 2.2500358179601934\n",
      "Epoch: 5/100, Iteration: 301/450, Loss: 2.2751440483710135\n",
      "Epoch: 5/100, Iteration: 401/450, Loss: 2.268312357893382\n",
      "Epoch: 5/100, Iteration: 450/450, Loss: 2.2942074605896785\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 5/100, Train accuracy: 0.167, Test accuracy: 0.14826666666666666\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 6/100, Iteration: 1/450, Loss: 2.277929305737508\n",
      "Epoch: 6/100, Iteration: 101/450, Loss: 2.2630925458572655\n",
      "Epoch: 6/100, Iteration: 201/450, Loss: 2.251563535353643\n",
      "Epoch: 6/100, Iteration: 301/450, Loss: 2.215219405923273\n",
      "Epoch: 6/100, Iteration: 401/450, Loss: 2.2752237158733895\n",
      "Epoch: 6/100, Iteration: 450/450, Loss: 2.2935238083922673\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 6/100, Train accuracy: 0.16775555555555555, Test accuracy: 0.1672\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 7/100, Iteration: 1/450, Loss: 2.2590201651976467\n",
      "Epoch: 7/100, Iteration: 101/450, Loss: 2.220079889445304\n",
      "Epoch: 7/100, Iteration: 201/450, Loss: 2.2480250712580476\n",
      "Epoch: 7/100, Iteration: 301/450, Loss: 2.227873533343894\n",
      "Epoch: 7/100, Iteration: 401/450, Loss: 2.257130970707698\n",
      "Epoch: 7/100, Iteration: 450/450, Loss: 2.2001583182781888\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 7/100, Train accuracy: 0.24093333333333333, Test accuracy: 0.21506666666666666\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 8/100, Iteration: 1/450, Loss: 2.235691486104345\n",
      "Epoch: 8/100, Iteration: 101/450, Loss: 2.24312934676833\n",
      "Epoch: 8/100, Iteration: 201/450, Loss: 2.208384515589857\n",
      "Epoch: 8/100, Iteration: 301/450, Loss: 2.2317497067398686\n",
      "Epoch: 8/100, Iteration: 401/450, Loss: 2.1521964845054704\n",
      "Epoch: 8/100, Iteration: 450/450, Loss: 2.173612577642249\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 8/100, Train accuracy: 0.2322, Test accuracy: 0.2286\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 9/100, Iteration: 1/450, Loss: 2.183337403899667\n",
      "Epoch: 9/100, Iteration: 101/450, Loss: 2.2371789198062633\n",
      "Epoch: 9/100, Iteration: 201/450, Loss: 2.1503177283002035\n",
      "Epoch: 9/100, Iteration: 301/450, Loss: 2.159004407334967\n",
      "Epoch: 9/100, Iteration: 401/450, Loss: 2.16602146349559\n",
      "Epoch: 9/100, Iteration: 450/450, Loss: 2.0980872030681947\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 9/100, Train accuracy: 0.2870666666666667, Test accuracy: 0.281\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 10/100, Iteration: 1/450, Loss: 2.113941064101608\n",
      "Epoch: 10/100, Iteration: 101/450, Loss: 2.107634480439477\n",
      "Epoch: 10/100, Iteration: 201/450, Loss: 2.1297351854025535\n",
      "Epoch: 10/100, Iteration: 301/450, Loss: 2.0177117522246\n",
      "Epoch: 10/100, Iteration: 401/450, Loss: 2.070123049466293\n",
      "Epoch: 10/100, Iteration: 450/450, Loss: 2.1128994355294886\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 10/100, Train accuracy: 0.3267111111111111, Test accuracy: 0.30086666666666667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 11/100, Iteration: 1/450, Loss: 1.9396306029873762\n",
      "Epoch: 11/100, Iteration: 101/450, Loss: 2.043891709568937\n",
      "Epoch: 11/100, Iteration: 201/450, Loss: 1.9736008257238493\n",
      "Epoch: 11/100, Iteration: 301/450, Loss: 1.9045617328782187\n",
      "Epoch: 11/100, Iteration: 401/450, Loss: 2.049124825122198\n",
      "Epoch: 11/100, Iteration: 450/450, Loss: 2.0031549078328728\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 11/100, Train accuracy: 0.3476888888888889, Test accuracy: 0.32206666666666667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 12/100, Iteration: 1/450, Loss: 1.8901375402806635\n",
      "Epoch: 12/100, Iteration: 101/450, Loss: 1.99337495070059\n",
      "Epoch: 12/100, Iteration: 201/450, Loss: 1.9158541988106623\n",
      "Epoch: 12/100, Iteration: 301/450, Loss: 2.025036906861375\n",
      "Epoch: 12/100, Iteration: 401/450, Loss: 1.8736571236750756\n",
      "Epoch: 12/100, Iteration: 450/450, Loss: 1.9528760626783657\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 12/100, Train accuracy: 0.37335555555555555, Test accuracy: 0.33013333333333333\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 13/100, Iteration: 1/450, Loss: 1.8033371430655603\n",
      "Epoch: 13/100, Iteration: 101/450, Loss: 1.841948207666914\n",
      "Epoch: 13/100, Iteration: 201/450, Loss: 1.9500731088375525\n",
      "Epoch: 13/100, Iteration: 301/450, Loss: 1.9040123129729085\n",
      "Epoch: 13/100, Iteration: 401/450, Loss: 1.8408985584283928\n",
      "Epoch: 13/100, Iteration: 450/450, Loss: 1.825189402407506\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 13/100, Train accuracy: 0.40942222222222224, Test accuracy: 0.3977333333333333\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 14/100, Iteration: 1/450, Loss: 1.8928338337143849\n",
      "Epoch: 14/100, Iteration: 101/450, Loss: 1.8378646747036245\n",
      "Epoch: 14/100, Iteration: 201/450, Loss: 1.789175416606696\n",
      "Epoch: 14/100, Iteration: 301/450, Loss: 1.7520786847431538\n",
      "Epoch: 14/100, Iteration: 401/450, Loss: 1.760146454462008\n",
      "Epoch: 14/100, Iteration: 450/450, Loss: 1.7646342154642747\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 14/100, Train accuracy: 0.42204444444444444, Test accuracy: 0.39813333333333334\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 15/100, Iteration: 1/450, Loss: 1.8407094554898296\n",
      "Epoch: 15/100, Iteration: 101/450, Loss: 1.8552848502242782\n",
      "Epoch: 15/100, Iteration: 201/450, Loss: 1.6920512321808467\n",
      "Epoch: 15/100, Iteration: 301/450, Loss: 1.7027872394715187\n",
      "Epoch: 15/100, Iteration: 401/450, Loss: 1.714070784547792\n",
      "Epoch: 15/100, Iteration: 450/450, Loss: 1.7232311825052233\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 15/100, Train accuracy: 0.43937777777777776, Test accuracy: 0.4040666666666667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 16/100, Iteration: 1/450, Loss: 1.6111446377946501\n",
      "Epoch: 16/100, Iteration: 101/450, Loss: 1.7133411418934947\n",
      "Epoch: 16/100, Iteration: 201/450, Loss: 1.7506177588138638\n",
      "Epoch: 16/100, Iteration: 301/450, Loss: 1.4973977586594907\n",
      "Epoch: 16/100, Iteration: 401/450, Loss: 1.6341903146379437\n",
      "Epoch: 16/100, Iteration: 450/450, Loss: 1.7281989506715825\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 16/100, Train accuracy: 0.4878222222222222, Test accuracy: 0.45206666666666667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 17/100, Iteration: 1/450, Loss: 1.6537026514734288\n",
      "Epoch: 17/100, Iteration: 101/450, Loss: 1.7250039838085627\n",
      "Epoch: 17/100, Iteration: 201/450, Loss: 1.6950883152405314\n",
      "Epoch: 17/100, Iteration: 301/450, Loss: 1.6509785791613383\n",
      "Epoch: 17/100, Iteration: 401/450, Loss: 1.6791983069736847\n",
      "Epoch: 17/100, Iteration: 450/450, Loss: 1.5357848157532852\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 17/100, Train accuracy: 0.5067777777777778, Test accuracy: 0.4716\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 18/100, Iteration: 1/450, Loss: 1.5786113290101849\n",
      "Epoch: 18/100, Iteration: 101/450, Loss: 1.7205182105975172\n",
      "Epoch: 18/100, Iteration: 201/450, Loss: 1.5091455588476648\n",
      "Epoch: 18/100, Iteration: 301/450, Loss: 1.6651010839026446\n",
      "Epoch: 18/100, Iteration: 401/450, Loss: 1.6528899433031647\n",
      "Epoch: 18/100, Iteration: 450/450, Loss: 1.642102216624926\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 18/100, Train accuracy: 0.5207555555555555, Test accuracy: 0.495\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 19/100, Iteration: 1/450, Loss: 1.5885464979325574\n",
      "Epoch: 19/100, Iteration: 101/450, Loss: 1.5138160344472291\n",
      "Epoch: 19/100, Iteration: 201/450, Loss: 1.5115534748608315\n",
      "Epoch: 19/100, Iteration: 301/450, Loss: 1.5895207681858095\n",
      "Epoch: 19/100, Iteration: 401/450, Loss: 1.5367687004608939\n",
      "Epoch: 19/100, Iteration: 450/450, Loss: 1.547248397592786\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 19/100, Train accuracy: 0.5118222222222222, Test accuracy: 0.481\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 20/100, Iteration: 1/450, Loss: 1.5613915095259505\n",
      "Epoch: 20/100, Iteration: 101/450, Loss: 1.4823970682532532\n",
      "Epoch: 20/100, Iteration: 201/450, Loss: 1.5082285939318092\n",
      "Epoch: 20/100, Iteration: 301/450, Loss: 1.375502708916731\n",
      "Epoch: 20/100, Iteration: 401/450, Loss: 1.5727553568402084\n",
      "Epoch: 20/100, Iteration: 450/450, Loss: 1.53199224662608\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 20/100, Train accuracy: 0.5477111111111111, Test accuracy: 0.5056666666666667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 21/100, Iteration: 1/450, Loss: 1.4579421461835667\n",
      "Epoch: 21/100, Iteration: 101/450, Loss: 1.5102841416478965\n",
      "Epoch: 21/100, Iteration: 201/450, Loss: 1.376132064442148\n",
      "Epoch: 21/100, Iteration: 301/450, Loss: 1.4228823753065336\n",
      "Epoch: 21/100, Iteration: 401/450, Loss: 1.5103272455838461\n",
      "Epoch: 21/100, Iteration: 450/450, Loss: 1.420621671797778\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 21/100, Train accuracy: 0.5695777777777777, Test accuracy: 0.5334666666666666\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 22/100, Iteration: 1/450, Loss: 1.457225488253418\n",
      "Epoch: 22/100, Iteration: 101/450, Loss: 1.439835867909715\n",
      "Epoch: 22/100, Iteration: 201/450, Loss: 1.5100144620727707\n",
      "Epoch: 22/100, Iteration: 301/450, Loss: 1.355321982858574\n",
      "Epoch: 22/100, Iteration: 401/450, Loss: 1.4117247377312225\n",
      "Epoch: 22/100, Iteration: 450/450, Loss: 1.4766386615500111\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 22/100, Train accuracy: 0.5689333333333333, Test accuracy: 0.5293333333333333\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 23/100, Iteration: 1/450, Loss: 1.4024604145196145\n",
      "Epoch: 23/100, Iteration: 101/450, Loss: 1.2531428931128132\n",
      "Epoch: 23/100, Iteration: 201/450, Loss: 1.5510409701083967\n",
      "Epoch: 23/100, Iteration: 301/450, Loss: 1.5081621077099843\n",
      "Epoch: 23/100, Iteration: 401/450, Loss: 1.561981208617175\n",
      "Epoch: 23/100, Iteration: 450/450, Loss: 1.4259060395241085\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 23/100, Train accuracy: 0.5693777777777778, Test accuracy: 0.5323333333333333\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 24/100, Iteration: 1/450, Loss: 1.4044426891782464\n",
      "Epoch: 24/100, Iteration: 101/450, Loss: 1.4653966967510064\n",
      "Epoch: 24/100, Iteration: 201/450, Loss: 1.3301858634202812\n",
      "Epoch: 24/100, Iteration: 301/450, Loss: 1.4309903095257583\n",
      "Epoch: 24/100, Iteration: 401/450, Loss: 1.4371065618463112\n",
      "Epoch: 24/100, Iteration: 450/450, Loss: 1.3735463039568978\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 24/100, Train accuracy: 0.5664222222222223, Test accuracy: 0.5360666666666667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 25/100, Iteration: 1/450, Loss: 1.464440877972586\n",
      "Epoch: 25/100, Iteration: 101/450, Loss: 1.3488645282667415\n",
      "Epoch: 25/100, Iteration: 201/450, Loss: 1.3538970962524945\n",
      "Epoch: 25/100, Iteration: 301/450, Loss: 1.3969689884965715\n",
      "Epoch: 25/100, Iteration: 401/450, Loss: 1.2871996553977312\n",
      "Epoch: 25/100, Iteration: 450/450, Loss: 1.2435450952307787\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 25/100, Train accuracy: 0.5780888888888889, Test accuracy: 0.5380666666666667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 26/100, Iteration: 1/450, Loss: 1.4218155126885283\n",
      "Epoch: 26/100, Iteration: 101/450, Loss: 1.5898902968018118\n",
      "Epoch: 26/100, Iteration: 201/450, Loss: 1.3343291620265898\n",
      "Epoch: 26/100, Iteration: 301/450, Loss: 1.3689460923094714\n",
      "Epoch: 26/100, Iteration: 401/450, Loss: 1.3546300619714027\n",
      "Epoch: 26/100, Iteration: 450/450, Loss: 1.2192379582801514\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 26/100, Train accuracy: 0.6059333333333333, Test accuracy: 0.5624666666666667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 27/100, Iteration: 1/450, Loss: 1.2973220561168677\n",
      "Epoch: 27/100, Iteration: 101/450, Loss: 1.3136237640725674\n",
      "Epoch: 27/100, Iteration: 201/450, Loss: 1.283322098047081\n",
      "Epoch: 27/100, Iteration: 301/450, Loss: 1.4343851441355704\n",
      "Epoch: 27/100, Iteration: 401/450, Loss: 1.194087999781491\n",
      "Epoch: 27/100, Iteration: 450/450, Loss: 1.2555006933836865\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 27/100, Train accuracy: 0.6163555555555555, Test accuracy: 0.5769333333333333\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 28/100, Iteration: 1/450, Loss: 1.437803024363354\n",
      "Epoch: 28/100, Iteration: 101/450, Loss: 1.2211187620046118\n",
      "Epoch: 28/100, Iteration: 201/450, Loss: 1.3832390250091209\n",
      "Epoch: 28/100, Iteration: 301/450, Loss: 1.2770227404411798\n",
      "Epoch: 28/100, Iteration: 401/450, Loss: 1.4193521736681043\n",
      "Epoch: 28/100, Iteration: 450/450, Loss: 1.2558556783781665\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 28/100, Train accuracy: 0.6178666666666667, Test accuracy: 0.5628\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 29/100, Iteration: 1/450, Loss: 1.2545812608358486\n",
      "Epoch: 29/100, Iteration: 101/450, Loss: 1.2190587770211283\n",
      "Epoch: 29/100, Iteration: 201/450, Loss: 1.1665418124045772\n",
      "Epoch: 29/100, Iteration: 301/450, Loss: 1.340173275061708\n",
      "Epoch: 29/100, Iteration: 401/450, Loss: 1.2774617544397608\n",
      "Epoch: 29/100, Iteration: 450/450, Loss: 1.2066969010436437\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 29/100, Train accuracy: 0.6315777777777778, Test accuracy: 0.5914666666666667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 30/100, Iteration: 1/450, Loss: 1.3717208901840103\n",
      "Epoch: 30/100, Iteration: 101/450, Loss: 1.2448148160321\n",
      "Epoch: 30/100, Iteration: 201/450, Loss: 1.3911808443165483\n",
      "Epoch: 30/100, Iteration: 301/450, Loss: 1.3969995262091914\n",
      "Epoch: 30/100, Iteration: 401/450, Loss: 1.2561228381910725\n",
      "Epoch: 30/100, Iteration: 450/450, Loss: 1.2610052754707628\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 30/100, Train accuracy: 0.6194666666666667, Test accuracy: 0.5753333333333334\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 31/100, Iteration: 1/450, Loss: 1.0785841333647053\n",
      "Epoch: 31/100, Iteration: 101/450, Loss: 1.4007990854187353\n",
      "Epoch: 31/100, Iteration: 201/450, Loss: 1.1442165083639335\n",
      "Epoch: 31/100, Iteration: 301/450, Loss: 1.1334467742611556\n",
      "Epoch: 31/100, Iteration: 401/450, Loss: 1.30161322785385\n",
      "Epoch: 31/100, Iteration: 450/450, Loss: 1.2185737423363374\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 31/100, Train accuracy: 0.6247555555555555, Test accuracy: 0.5842\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 32/100, Iteration: 1/450, Loss: 1.2836665438603303\n",
      "Epoch: 32/100, Iteration: 101/450, Loss: 1.2933788003801454\n",
      "Epoch: 32/100, Iteration: 201/450, Loss: 1.3578408000410351\n",
      "Epoch: 32/100, Iteration: 301/450, Loss: 1.2459251628150887\n",
      "Epoch: 32/100, Iteration: 401/450, Loss: 1.1980786006041932\n",
      "Epoch: 32/100, Iteration: 450/450, Loss: 1.1971623541527996\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 32/100, Train accuracy: 0.6357111111111111, Test accuracy: 0.589\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 33/100, Iteration: 1/450, Loss: 1.2606096152420903\n",
      "Epoch: 33/100, Iteration: 101/450, Loss: 1.1522215804286\n",
      "Epoch: 33/100, Iteration: 201/450, Loss: 1.1677015429030824\n",
      "Epoch: 33/100, Iteration: 301/450, Loss: 1.3106640017189193\n",
      "Epoch: 33/100, Iteration: 401/450, Loss: 1.2016880227537612\n",
      "Epoch: 33/100, Iteration: 450/450, Loss: 1.328807889338638\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 33/100, Train accuracy: 0.6496, Test accuracy: 0.6072666666666666\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 34/100, Iteration: 1/450, Loss: 1.1960447037982782\n",
      "Epoch: 34/100, Iteration: 101/450, Loss: 1.1976394640904509\n",
      "Epoch: 34/100, Iteration: 201/450, Loss: 1.3733165357250916\n",
      "Epoch: 34/100, Iteration: 301/450, Loss: 1.217400612405202\n",
      "Epoch: 34/100, Iteration: 401/450, Loss: 1.2667692263310386\n",
      "Epoch: 34/100, Iteration: 450/450, Loss: 1.1504468494025353\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 34/100, Train accuracy: 0.6440888888888889, Test accuracy: 0.5992\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 35/100, Iteration: 1/450, Loss: 1.3558973876432048\n",
      "Epoch: 35/100, Iteration: 101/450, Loss: 1.1869182079491376\n",
      "Epoch: 35/100, Iteration: 201/450, Loss: 1.3346220730656384\n",
      "Epoch: 35/100, Iteration: 301/450, Loss: 1.2332413272075888\n",
      "Epoch: 35/100, Iteration: 401/450, Loss: 1.137127194380023\n",
      "Epoch: 35/100, Iteration: 450/450, Loss: 1.2302876022793239\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 35/100, Train accuracy: 0.6466666666666666, Test accuracy: 0.5997333333333333\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 36/100, Iteration: 1/450, Loss: 1.1087655661676545\n",
      "Epoch: 36/100, Iteration: 101/450, Loss: 1.1882131622956482\n",
      "Epoch: 36/100, Iteration: 201/450, Loss: 1.2205543529756817\n",
      "Epoch: 36/100, Iteration: 301/450, Loss: 1.2489675665445774\n",
      "Epoch: 36/100, Iteration: 401/450, Loss: 1.020265717505913\n",
      "Epoch: 36/100, Iteration: 450/450, Loss: 1.1997869817433897\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 36/100, Train accuracy: 0.6362, Test accuracy: 0.5973333333333334\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 37/100, Iteration: 1/450, Loss: 1.392349073760177\n",
      "Epoch: 37/100, Iteration: 101/450, Loss: 1.303689742001676\n",
      "Epoch: 37/100, Iteration: 201/450, Loss: 1.3389560387333859\n",
      "Epoch: 37/100, Iteration: 301/450, Loss: 1.1339323514028692\n",
      "Epoch: 37/100, Iteration: 401/450, Loss: 1.0747450311014193\n",
      "Epoch: 37/100, Iteration: 450/450, Loss: 1.0408701327547993\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 37/100, Train accuracy: 0.6528888888888889, Test accuracy: 0.6014666666666667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 38/100, Iteration: 1/450, Loss: 1.1906243452734153\n",
      "Epoch: 38/100, Iteration: 101/450, Loss: 1.0276224051595306\n",
      "Epoch: 38/100, Iteration: 201/450, Loss: 1.2013146515202073\n",
      "Epoch: 38/100, Iteration: 301/450, Loss: 1.0556998428099333\n",
      "Epoch: 38/100, Iteration: 401/450, Loss: 1.0996765557155257\n",
      "Epoch: 38/100, Iteration: 450/450, Loss: 1.1305008775183556\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 38/100, Train accuracy: 0.6409777777777778, Test accuracy: 0.585\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 39/100, Iteration: 1/450, Loss: 1.19093529579827\n",
      "Epoch: 39/100, Iteration: 101/450, Loss: 1.510444955488078\n",
      "Epoch: 39/100, Iteration: 201/450, Loss: 1.1098939574952749\n",
      "Epoch: 39/100, Iteration: 301/450, Loss: 1.0667367322739496\n",
      "Epoch: 39/100, Iteration: 401/450, Loss: 1.2390439512184899\n",
      "Epoch: 39/100, Iteration: 450/450, Loss: 1.1857008964546327\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 39/100, Train accuracy: 0.6740222222222222, Test accuracy: 0.6244\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 40/100, Iteration: 1/450, Loss: 1.2323924958128223\n",
      "Epoch: 40/100, Iteration: 101/450, Loss: 1.3653253387150186\n",
      "Epoch: 40/100, Iteration: 201/450, Loss: 1.1121711806553152\n",
      "Epoch: 40/100, Iteration: 301/450, Loss: 1.1228027238852305\n",
      "Epoch: 40/100, Iteration: 401/450, Loss: 1.325065568857874\n",
      "Epoch: 40/100, Iteration: 450/450, Loss: 1.1308775778484594\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 40/100, Train accuracy: 0.6655777777777778, Test accuracy: 0.6198666666666667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 41/100, Iteration: 1/450, Loss: 1.1199461755475513\n",
      "Epoch: 41/100, Iteration: 101/450, Loss: 1.2408108593895608\n",
      "Epoch: 41/100, Iteration: 201/450, Loss: 1.0721940030163473\n",
      "Epoch: 41/100, Iteration: 301/450, Loss: 1.3922291026178448\n",
      "Epoch: 41/100, Iteration: 401/450, Loss: 1.2298548122191613\n",
      "Epoch: 41/100, Iteration: 450/450, Loss: 1.25106339350712\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 41/100, Train accuracy: 0.6613333333333333, Test accuracy: 0.6096666666666667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 42/100, Iteration: 1/450, Loss: 1.299395092035254\n",
      "Epoch: 42/100, Iteration: 101/450, Loss: 1.1652222996571693\n",
      "Epoch: 42/100, Iteration: 201/450, Loss: 1.1144343579468927\n",
      "Epoch: 42/100, Iteration: 301/450, Loss: 1.1582427067486198\n",
      "Epoch: 42/100, Iteration: 401/450, Loss: 0.9765657621066208\n",
      "Epoch: 42/100, Iteration: 450/450, Loss: 0.9449223918074787\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 42/100, Train accuracy: 0.6429111111111111, Test accuracy: 0.598\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 43/100, Iteration: 1/450, Loss: 1.2886465898341009\n",
      "Epoch: 43/100, Iteration: 101/450, Loss: 1.4389676918290462\n",
      "Epoch: 43/100, Iteration: 201/450, Loss: 1.1610663750392896\n",
      "Epoch: 43/100, Iteration: 301/450, Loss: 1.1425208162912421\n",
      "Epoch: 43/100, Iteration: 401/450, Loss: 1.2019905955239796\n",
      "Epoch: 43/100, Iteration: 450/450, Loss: 1.2777064034397423\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 43/100, Train accuracy: 0.6628222222222222, Test accuracy: 0.6103333333333333\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 44/100, Iteration: 1/450, Loss: 0.9591353283651549\n",
      "Epoch: 44/100, Iteration: 101/450, Loss: 1.1937624600692294\n",
      "Epoch: 44/100, Iteration: 201/450, Loss: 1.245912137876282\n",
      "Epoch: 44/100, Iteration: 301/450, Loss: 1.060887411659863\n",
      "Epoch: 44/100, Iteration: 401/450, Loss: 1.2451665194095112\n",
      "Epoch: 44/100, Iteration: 450/450, Loss: 1.1561058451662\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 44/100, Train accuracy: 0.6590888888888888, Test accuracy: 0.6123333333333333\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 45/100, Iteration: 1/450, Loss: 1.092188666525723\n",
      "Epoch: 45/100, Iteration: 101/450, Loss: 1.189318536823527\n",
      "Epoch: 45/100, Iteration: 201/450, Loss: 0.9885392913734512\n",
      "Epoch: 45/100, Iteration: 301/450, Loss: 1.1765571781701893\n",
      "Epoch: 45/100, Iteration: 401/450, Loss: 1.2712480789396547\n",
      "Epoch: 45/100, Iteration: 450/450, Loss: 1.2940744836543063\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 45/100, Train accuracy: 0.6600888888888888, Test accuracy: 0.6250666666666667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 46/100, Iteration: 1/450, Loss: 0.9377846603366788\n",
      "Epoch: 46/100, Iteration: 101/450, Loss: 1.173519546934427\n",
      "Epoch: 46/100, Iteration: 201/450, Loss: 1.2539436149763368\n",
      "Epoch: 46/100, Iteration: 301/450, Loss: 1.1088902428585903\n",
      "Epoch: 46/100, Iteration: 401/450, Loss: 1.1943457612436759\n",
      "Epoch: 46/100, Iteration: 450/450, Loss: 1.1444454539106959\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 46/100, Train accuracy: 0.6856, Test accuracy: 0.6304\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 47/100, Iteration: 1/450, Loss: 1.1096053646170754\n",
      "Epoch: 47/100, Iteration: 101/450, Loss: 1.0050174730354096\n",
      "Epoch: 47/100, Iteration: 201/450, Loss: 1.2099819473313673\n",
      "Epoch: 47/100, Iteration: 301/450, Loss: 1.1123905553082212\n",
      "Epoch: 47/100, Iteration: 401/450, Loss: 1.0650534741416318\n",
      "Epoch: 47/100, Iteration: 450/450, Loss: 1.2620593378620113\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 47/100, Train accuracy: 0.6656666666666666, Test accuracy: 0.6202666666666666\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 48/100, Iteration: 1/450, Loss: 1.002753338257775\n",
      "Epoch: 48/100, Iteration: 101/450, Loss: 1.3461942026230145\n",
      "Epoch: 48/100, Iteration: 201/450, Loss: 1.0862007151622175\n",
      "Epoch: 48/100, Iteration: 301/450, Loss: 1.1733162587368846\n",
      "Epoch: 48/100, Iteration: 401/450, Loss: 0.9799572157841077\n",
      "Epoch: 48/100, Iteration: 450/450, Loss: 1.0932160318252824\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 48/100, Train accuracy: 0.6663111111111111, Test accuracy: 0.6219333333333333\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 49/100, Iteration: 1/450, Loss: 1.2412135812783007\n",
      "Epoch: 49/100, Iteration: 101/450, Loss: 1.1320574324747892\n",
      "Epoch: 49/100, Iteration: 201/450, Loss: 1.1350835988367465\n",
      "Epoch: 49/100, Iteration: 301/450, Loss: 1.289270953630828\n",
      "Epoch: 49/100, Iteration: 401/450, Loss: 1.2109152698280379\n",
      "Epoch: 49/100, Iteration: 450/450, Loss: 0.9435243157153539\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 49/100, Train accuracy: 0.6672444444444444, Test accuracy: 0.6252666666666666\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 50/100, Iteration: 1/450, Loss: 1.0928518339346476\n",
      "Epoch: 50/100, Iteration: 101/450, Loss: 1.2370072098474578\n",
      "Epoch: 50/100, Iteration: 201/450, Loss: 0.9534929968099083\n",
      "Epoch: 50/100, Iteration: 301/450, Loss: 1.1719070384662111\n",
      "Epoch: 50/100, Iteration: 401/450, Loss: 1.0970366967602834\n",
      "Epoch: 50/100, Iteration: 450/450, Loss: 1.1144116574940994\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 50/100, Train accuracy: 0.6469555555555555, Test accuracy: 0.6036\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 51/100, Iteration: 1/450, Loss: 1.2746284480833197\n",
      "Epoch: 51/100, Iteration: 101/450, Loss: 0.944459050977511\n",
      "Epoch: 51/100, Iteration: 201/450, Loss: 0.9227312875066628\n",
      "Epoch: 51/100, Iteration: 301/450, Loss: 1.1781369875890781\n",
      "Epoch: 51/100, Iteration: 401/450, Loss: 1.1726301389841267\n",
      "Epoch: 51/100, Iteration: 450/450, Loss: 1.086568064017405\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 51/100, Train accuracy: 0.6640444444444444, Test accuracy: 0.6126666666666667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 52/100, Iteration: 1/450, Loss: 1.205307502109016\n",
      "Epoch: 52/100, Iteration: 101/450, Loss: 1.0925333309715781\n",
      "Epoch: 52/100, Iteration: 201/450, Loss: 1.1981858231518892\n",
      "Epoch: 52/100, Iteration: 301/450, Loss: 1.1460588689066509\n",
      "Epoch: 52/100, Iteration: 401/450, Loss: 1.1153955193921634\n",
      "Epoch: 52/100, Iteration: 450/450, Loss: 1.1329735584101115\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 52/100, Train accuracy: 0.6834, Test accuracy: 0.6298666666666667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 53/100, Iteration: 1/450, Loss: 1.0434046619842487\n",
      "Epoch: 53/100, Iteration: 101/450, Loss: 0.9695868646035625\n",
      "Epoch: 53/100, Iteration: 201/450, Loss: 1.12122931237262\n",
      "Epoch: 53/100, Iteration: 301/450, Loss: 1.0703363301702407\n",
      "Epoch: 53/100, Iteration: 401/450, Loss: 1.1365554065457246\n",
      "Epoch: 53/100, Iteration: 450/450, Loss: 0.9585432967151352\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 53/100, Train accuracy: 0.6751555555555555, Test accuracy: 0.6304\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 54/100, Iteration: 1/450, Loss: 1.1950963006346311\n",
      "Epoch: 54/100, Iteration: 101/450, Loss: 1.1685466708216392\n",
      "Epoch: 54/100, Iteration: 201/450, Loss: 0.9338206972532223\n",
      "Epoch: 54/100, Iteration: 301/450, Loss: 1.1458527524093443\n",
      "Epoch: 54/100, Iteration: 401/450, Loss: 1.076233270842839\n",
      "Epoch: 54/100, Iteration: 450/450, Loss: 0.9969218489701052\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 54/100, Train accuracy: 0.6843777777777778, Test accuracy: 0.6310666666666667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 55/100, Iteration: 1/450, Loss: 1.0052806095602644\n",
      "Epoch: 55/100, Iteration: 101/450, Loss: 1.1041967249976392\n",
      "Epoch: 55/100, Iteration: 201/450, Loss: 1.1227243223987482\n",
      "Epoch: 55/100, Iteration: 301/450, Loss: 1.0093339163281825\n",
      "Epoch: 55/100, Iteration: 401/450, Loss: 1.0164296936409978\n",
      "Epoch: 55/100, Iteration: 450/450, Loss: 1.0956823129347308\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 55/100, Train accuracy: 0.6772444444444444, Test accuracy: 0.6336\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 56/100, Iteration: 1/450, Loss: 0.9461688891796716\n",
      "Epoch: 56/100, Iteration: 101/450, Loss: 1.10760741854936\n",
      "Epoch: 56/100, Iteration: 201/450, Loss: 1.1245977377800025\n",
      "Epoch: 56/100, Iteration: 301/450, Loss: 1.3027545683573374\n",
      "Epoch: 56/100, Iteration: 401/450, Loss: 1.0094616014372144\n",
      "Epoch: 56/100, Iteration: 450/450, Loss: 1.2005139761028196\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 56/100, Train accuracy: 0.6811777777777778, Test accuracy: 0.6374\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 57/100, Iteration: 1/450, Loss: 1.0464106771232016\n",
      "Epoch: 57/100, Iteration: 101/450, Loss: 1.0969935659140946\n",
      "Epoch: 57/100, Iteration: 201/450, Loss: 0.9706896965801468\n",
      "Epoch: 57/100, Iteration: 301/450, Loss: 1.2256136060603608\n",
      "Epoch: 57/100, Iteration: 401/450, Loss: 1.0165537151037207\n",
      "Epoch: 57/100, Iteration: 450/450, Loss: 0.9979627048078532\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 57/100, Train accuracy: 0.6865333333333333, Test accuracy: 0.6364\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 58/100, Iteration: 1/450, Loss: 1.0276294249642337\n",
      "Epoch: 58/100, Iteration: 101/450, Loss: 1.0924841869391346\n",
      "Epoch: 58/100, Iteration: 201/450, Loss: 1.2849777123683337\n",
      "Epoch: 58/100, Iteration: 301/450, Loss: 1.1102615473897466\n",
      "Epoch: 58/100, Iteration: 401/450, Loss: 1.1059993918715816\n",
      "Epoch: 58/100, Iteration: 450/450, Loss: 1.0688541085871928\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 58/100, Train accuracy: 0.69, Test accuracy: 0.6400666666666667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 59/100, Iteration: 1/450, Loss: 1.1625738707916329\n",
      "Epoch: 59/100, Iteration: 101/450, Loss: 0.9871506749193552\n",
      "Epoch: 59/100, Iteration: 201/450, Loss: 1.059214563717787\n",
      "Epoch: 59/100, Iteration: 301/450, Loss: 1.0039088588126042\n",
      "Epoch: 59/100, Iteration: 401/450, Loss: 1.1250209402356484\n",
      "Epoch: 59/100, Iteration: 450/450, Loss: 1.1215074924138253\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 59/100, Train accuracy: 0.6759777777777778, Test accuracy: 0.6316666666666667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 60/100, Iteration: 1/450, Loss: 1.2201317284361302\n",
      "Epoch: 60/100, Iteration: 101/450, Loss: 1.0729032388557556\n",
      "Epoch: 60/100, Iteration: 201/450, Loss: 1.1187087776920166\n",
      "Epoch: 60/100, Iteration: 301/450, Loss: 1.3018947090974473\n",
      "Epoch: 60/100, Iteration: 401/450, Loss: 1.0671746523893808\n",
      "Epoch: 60/100, Iteration: 450/450, Loss: 1.230544981364266\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 60/100, Train accuracy: 0.6781111111111111, Test accuracy: 0.6256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 61/100, Iteration: 1/450, Loss: 1.0036080018411646\n",
      "Epoch: 61/100, Iteration: 101/450, Loss: 1.1445605392933689\n",
      "Epoch: 61/100, Iteration: 201/450, Loss: 1.1245661912163485\n",
      "Epoch: 61/100, Iteration: 301/450, Loss: 0.9168316058343825\n",
      "Epoch: 61/100, Iteration: 401/450, Loss: 1.0385417781373616\n",
      "Epoch: 61/100, Iteration: 450/450, Loss: 1.129986526465689\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 61/100, Train accuracy: 0.6850222222222222, Test accuracy: 0.6283333333333333\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 62/100, Iteration: 1/450, Loss: 1.0356685985145442\n",
      "Epoch: 62/100, Iteration: 101/450, Loss: 1.2100097782794716\n",
      "Epoch: 62/100, Iteration: 201/450, Loss: 1.290558832701812\n",
      "Epoch: 62/100, Iteration: 301/450, Loss: 1.0423636859503176\n",
      "Epoch: 62/100, Iteration: 401/450, Loss: 1.1608583863571944\n",
      "Epoch: 62/100, Iteration: 450/450, Loss: 1.0517161343810495\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 62/100, Train accuracy: 0.6945555555555556, Test accuracy: 0.6472\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 63/100, Iteration: 1/450, Loss: 1.325721012800713\n",
      "Epoch: 63/100, Iteration: 101/450, Loss: 1.1355028870626545\n",
      "Epoch: 63/100, Iteration: 201/450, Loss: 1.0567550737941855\n",
      "Epoch: 63/100, Iteration: 301/450, Loss: 1.2549307933274811\n",
      "Epoch: 63/100, Iteration: 401/450, Loss: 1.0074736223893874\n",
      "Epoch: 63/100, Iteration: 450/450, Loss: 1.0519323778626244\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 63/100, Train accuracy: 0.6596222222222222, Test accuracy: 0.6264666666666666\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 64/100, Iteration: 1/450, Loss: 1.2331191859433301\n",
      "Epoch: 64/100, Iteration: 101/450, Loss: 1.0603060119590488\n",
      "Epoch: 64/100, Iteration: 201/450, Loss: 1.2109310576917118\n",
      "Epoch: 64/100, Iteration: 301/450, Loss: 1.0849500989521348\n",
      "Epoch: 64/100, Iteration: 401/450, Loss: 1.1266345699081426\n",
      "Epoch: 64/100, Iteration: 450/450, Loss: 1.1342503949745708\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 64/100, Train accuracy: 0.6954888888888889, Test accuracy: 0.6424666666666666\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 65/100, Iteration: 1/450, Loss: 0.8988925339069034\n",
      "Epoch: 65/100, Iteration: 101/450, Loss: 1.1806017010733851\n",
      "Epoch: 65/100, Iteration: 201/450, Loss: 0.9731377562371044\n",
      "Epoch: 65/100, Iteration: 301/450, Loss: 1.162162245172833\n",
      "Epoch: 65/100, Iteration: 401/450, Loss: 0.9821581453847401\n",
      "Epoch: 65/100, Iteration: 450/450, Loss: 0.9365359328232573\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 65/100, Train accuracy: 0.7053333333333334, Test accuracy: 0.6510666666666667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 66/100, Iteration: 1/450, Loss: 0.9564941515260119\n",
      "Epoch: 66/100, Iteration: 101/450, Loss: 1.021528943839047\n",
      "Epoch: 66/100, Iteration: 201/450, Loss: 1.0796107855608508\n",
      "Epoch: 66/100, Iteration: 301/450, Loss: 1.0851567751737283\n",
      "Epoch: 66/100, Iteration: 401/450, Loss: 1.045631891816945\n",
      "Epoch: 66/100, Iteration: 450/450, Loss: 1.0758259638839234\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 66/100, Train accuracy: 0.6936444444444444, Test accuracy: 0.6376666666666667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 67/100, Iteration: 1/450, Loss: 1.0304304293984552\n",
      "Epoch: 67/100, Iteration: 101/450, Loss: 1.0529852657695922\n",
      "Epoch: 67/100, Iteration: 201/450, Loss: 1.1807530260441281\n",
      "Epoch: 67/100, Iteration: 301/450, Loss: 0.9677922917481052\n",
      "Epoch: 67/100, Iteration: 401/450, Loss: 1.0474181799057403\n",
      "Epoch: 67/100, Iteration: 450/450, Loss: 1.0185721624185464\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 67/100, Train accuracy: 0.6821777777777778, Test accuracy: 0.6327333333333334\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 68/100, Iteration: 1/450, Loss: 0.9701737192864462\n",
      "Epoch: 68/100, Iteration: 101/450, Loss: 1.0168310991259681\n",
      "Epoch: 68/100, Iteration: 201/450, Loss: 1.0249920381508004\n",
      "Epoch: 68/100, Iteration: 301/450, Loss: 0.980346462880845\n",
      "Epoch: 68/100, Iteration: 401/450, Loss: 1.1007736937985484\n",
      "Epoch: 68/100, Iteration: 450/450, Loss: 1.0904519553501268\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 68/100, Train accuracy: 0.6860444444444445, Test accuracy: 0.6358666666666667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 69/100, Iteration: 1/450, Loss: 0.9907248488442572\n",
      "Epoch: 69/100, Iteration: 101/450, Loss: 0.9577792008691961\n",
      "Epoch: 69/100, Iteration: 201/450, Loss: 0.8925086186186157\n",
      "Epoch: 69/100, Iteration: 301/450, Loss: 1.1717557588022947\n",
      "Epoch: 69/100, Iteration: 401/450, Loss: 0.9264727028248808\n",
      "Epoch: 69/100, Iteration: 450/450, Loss: 1.0242422629759744\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 69/100, Train accuracy: 0.7030444444444445, Test accuracy: 0.6526666666666666\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 70/100, Iteration: 1/450, Loss: 1.2042561344600242\n",
      "Epoch: 70/100, Iteration: 101/450, Loss: 0.9562652437382863\n",
      "Epoch: 70/100, Iteration: 201/450, Loss: 1.1088718767349437\n",
      "Epoch: 70/100, Iteration: 301/450, Loss: 0.9633134298200138\n",
      "Epoch: 70/100, Iteration: 401/450, Loss: 0.9988138693741737\n",
      "Epoch: 70/100, Iteration: 450/450, Loss: 1.1221681822736351\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 70/100, Train accuracy: 0.6926, Test accuracy: 0.6412\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 71/100, Iteration: 1/450, Loss: 1.0858449305620743\n",
      "Epoch: 71/100, Iteration: 101/450, Loss: 1.033829747980725\n",
      "Epoch: 71/100, Iteration: 201/450, Loss: 1.089706910276122\n",
      "Epoch: 71/100, Iteration: 301/450, Loss: 1.0968760491958782\n",
      "Epoch: 71/100, Iteration: 401/450, Loss: 1.20821257085059\n",
      "Epoch: 71/100, Iteration: 450/450, Loss: 1.0273360160343625\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 71/100, Train accuracy: 0.6873333333333334, Test accuracy: 0.6408666666666667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 72/100, Iteration: 1/450, Loss: 0.9493203913691818\n",
      "Epoch: 72/100, Iteration: 101/450, Loss: 1.092806912631519\n",
      "Epoch: 72/100, Iteration: 201/450, Loss: 1.018587075867471\n",
      "Epoch: 72/100, Iteration: 301/450, Loss: 1.1716671004175094\n",
      "Epoch: 72/100, Iteration: 401/450, Loss: 0.9443423464250634\n",
      "Epoch: 72/100, Iteration: 450/450, Loss: 1.1041589674054852\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 72/100, Train accuracy: 0.6946, Test accuracy: 0.6445333333333333\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 73/100, Iteration: 1/450, Loss: 1.1974823775349535\n",
      "Epoch: 73/100, Iteration: 101/450, Loss: 0.96224196170988\n",
      "Epoch: 73/100, Iteration: 201/450, Loss: 1.2421245879166278\n",
      "Epoch: 73/100, Iteration: 301/450, Loss: 1.0046762541520038\n",
      "Epoch: 73/100, Iteration: 401/450, Loss: 1.0674198176012666\n",
      "Epoch: 73/100, Iteration: 450/450, Loss: 1.0431459852051237\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 73/100, Train accuracy: 0.6929333333333333, Test accuracy: 0.6466666666666666\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 74/100, Iteration: 1/450, Loss: 0.9338911143920732\n",
      "Epoch: 74/100, Iteration: 101/450, Loss: 0.9628921403478202\n",
      "Epoch: 74/100, Iteration: 201/450, Loss: 1.2720008911289664\n",
      "Epoch: 74/100, Iteration: 301/450, Loss: 1.0471097225189667\n",
      "Epoch: 74/100, Iteration: 401/450, Loss: 1.0731070220769094\n",
      "Epoch: 74/100, Iteration: 450/450, Loss: 1.2854685289278223\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 74/100, Train accuracy: 0.6942444444444444, Test accuracy: 0.6342666666666666\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 75/100, Iteration: 1/450, Loss: 1.0356647770631506\n",
      "Epoch: 75/100, Iteration: 101/450, Loss: 1.0108108256673527\n",
      "Epoch: 75/100, Iteration: 201/450, Loss: 0.9848576886760179\n",
      "Epoch: 75/100, Iteration: 301/450, Loss: 1.1541702699300598\n",
      "Epoch: 75/100, Iteration: 401/450, Loss: 1.232173449853113\n",
      "Epoch: 75/100, Iteration: 450/450, Loss: 0.9686558252887838\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 75/100, Train accuracy: 0.6959111111111111, Test accuracy: 0.6445333333333333\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 76/100, Iteration: 1/450, Loss: 1.0242069253421509\n",
      "Epoch: 76/100, Iteration: 101/450, Loss: 1.1693486149735468\n",
      "Epoch: 76/100, Iteration: 201/450, Loss: 1.0749269248874425\n",
      "Epoch: 76/100, Iteration: 301/450, Loss: 1.1378369649255824\n",
      "Epoch: 76/100, Iteration: 401/450, Loss: 1.007875800712621\n",
      "Epoch: 76/100, Iteration: 450/450, Loss: 1.1277518311311148\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 76/100, Train accuracy: 0.7027555555555556, Test accuracy: 0.6555333333333333\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 77/100, Iteration: 1/450, Loss: 1.212385579749036\n",
      "Epoch: 77/100, Iteration: 101/450, Loss: 1.1410332078399985\n",
      "Epoch: 77/100, Iteration: 201/450, Loss: 0.9021196028605567\n",
      "Epoch: 77/100, Iteration: 301/450, Loss: 1.03433177952304\n",
      "Epoch: 77/100, Iteration: 401/450, Loss: 0.9886178308463708\n",
      "Epoch: 77/100, Iteration: 450/450, Loss: 0.8628435714104757\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 77/100, Train accuracy: 0.7092222222222222, Test accuracy: 0.659\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 78/100, Iteration: 1/450, Loss: 1.038524624790018\n",
      "Epoch: 78/100, Iteration: 101/450, Loss: 0.9617600889473101\n",
      "Epoch: 78/100, Iteration: 201/450, Loss: 0.7977575677184059\n",
      "Epoch: 78/100, Iteration: 301/450, Loss: 0.9787754099747258\n",
      "Epoch: 78/100, Iteration: 401/450, Loss: 1.153031518314657\n",
      "Epoch: 78/100, Iteration: 450/450, Loss: 0.7793077834269252\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 78/100, Train accuracy: 0.6973111111111111, Test accuracy: 0.643\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 79/100, Iteration: 1/450, Loss: 1.0917864083817015\n",
      "Epoch: 79/100, Iteration: 101/450, Loss: 0.9512058360161675\n",
      "Epoch: 79/100, Iteration: 201/450, Loss: 1.1143190328633765\n",
      "Epoch: 79/100, Iteration: 301/450, Loss: 0.9943904421076424\n",
      "Epoch: 79/100, Iteration: 401/450, Loss: 1.1082398561277673\n",
      "Epoch: 79/100, Iteration: 450/450, Loss: 1.0701730019191884\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 79/100, Train accuracy: 0.6978444444444445, Test accuracy: 0.6488\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 80/100, Iteration: 1/450, Loss: 1.120028763269507\n",
      "Epoch: 80/100, Iteration: 101/450, Loss: 0.9367224731521071\n",
      "Epoch: 80/100, Iteration: 201/450, Loss: 1.107840323383882\n",
      "Epoch: 80/100, Iteration: 301/450, Loss: 1.0197120190904294\n",
      "Epoch: 80/100, Iteration: 401/450, Loss: 1.0677253743582071\n",
      "Epoch: 80/100, Iteration: 450/450, Loss: 0.9534318244385294\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 80/100, Train accuracy: 0.7116222222222223, Test accuracy: 0.6636\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 81/100, Iteration: 1/450, Loss: 1.0332085390668901\n",
      "Epoch: 81/100, Iteration: 101/450, Loss: 0.8583719862463087\n",
      "Epoch: 81/100, Iteration: 201/450, Loss: 0.9494814480882225\n",
      "Epoch: 81/100, Iteration: 301/450, Loss: 1.1234006012508113\n",
      "Epoch: 81/100, Iteration: 401/450, Loss: 1.0556226879043442\n",
      "Epoch: 81/100, Iteration: 450/450, Loss: 1.0299502176048414\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 81/100, Train accuracy: 0.6968444444444445, Test accuracy: 0.6546666666666666\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 82/100, Iteration: 1/450, Loss: 1.0206922899234199\n",
      "Epoch: 82/100, Iteration: 101/450, Loss: 0.9947651248450022\n",
      "Epoch: 82/100, Iteration: 201/450, Loss: 1.0527344319709155\n",
      "Epoch: 82/100, Iteration: 301/450, Loss: 0.9882845458855138\n",
      "Epoch: 82/100, Iteration: 401/450, Loss: 0.901990898749066\n",
      "Epoch: 82/100, Iteration: 450/450, Loss: 0.8110493142067015\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 82/100, Train accuracy: 0.7072, Test accuracy: 0.6626\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 83/100, Iteration: 1/450, Loss: 1.010741731076484\n",
      "Epoch: 83/100, Iteration: 101/450, Loss: 0.9985736006810739\n",
      "Epoch: 83/100, Iteration: 201/450, Loss: 1.013818369210117\n",
      "Epoch: 83/100, Iteration: 301/450, Loss: 0.9307845456394718\n",
      "Epoch: 83/100, Iteration: 401/450, Loss: 1.0487293863675342\n",
      "Epoch: 83/100, Iteration: 450/450, Loss: 1.1865410620323484\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 83/100, Train accuracy: 0.6992888888888888, Test accuracy: 0.6400666666666667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 84/100, Iteration: 1/450, Loss: 1.0547109477636538\n",
      "Epoch: 84/100, Iteration: 101/450, Loss: 0.9670102196794715\n",
      "Epoch: 84/100, Iteration: 201/450, Loss: 0.9452374054915395\n",
      "Epoch: 84/100, Iteration: 301/450, Loss: 1.0373798464025805\n",
      "Epoch: 84/100, Iteration: 401/450, Loss: 0.958690786926219\n",
      "Epoch: 84/100, Iteration: 450/450, Loss: 0.9656717916838358\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 84/100, Train accuracy: 0.7148222222222222, Test accuracy: 0.6641333333333334\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 85/100, Iteration: 1/450, Loss: 0.8492105645391623\n",
      "Epoch: 85/100, Iteration: 101/450, Loss: 0.9580753400909218\n",
      "Epoch: 85/100, Iteration: 201/450, Loss: 1.1114516830681864\n",
      "Epoch: 85/100, Iteration: 301/450, Loss: 0.9740806768431761\n",
      "Epoch: 85/100, Iteration: 401/450, Loss: 0.9851732062767253\n",
      "Epoch: 85/100, Iteration: 450/450, Loss: 1.0493222222285474\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 85/100, Train accuracy: 0.7141777777777778, Test accuracy: 0.6599333333333334\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 86/100, Iteration: 1/450, Loss: 0.9398089684450602\n",
      "Epoch: 86/100, Iteration: 101/450, Loss: 0.9577123426612775\n",
      "Epoch: 86/100, Iteration: 201/450, Loss: 1.198635931571285\n",
      "Epoch: 86/100, Iteration: 301/450, Loss: 0.9487809698940234\n",
      "Epoch: 86/100, Iteration: 401/450, Loss: 0.9553064377946984\n",
      "Epoch: 86/100, Iteration: 450/450, Loss: 0.8141781439812877\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 86/100, Train accuracy: 0.6987111111111111, Test accuracy: 0.6396\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 87/100, Iteration: 1/450, Loss: 0.921633141715156\n",
      "Epoch: 87/100, Iteration: 101/450, Loss: 0.9494162058496852\n",
      "Epoch: 87/100, Iteration: 201/450, Loss: 1.040515126456842\n",
      "Epoch: 87/100, Iteration: 301/450, Loss: 0.9370162216320955\n",
      "Epoch: 87/100, Iteration: 401/450, Loss: 0.9076766126079661\n",
      "Epoch: 87/100, Iteration: 450/450, Loss: 1.157213248739592\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 87/100, Train accuracy: 0.7066666666666667, Test accuracy: 0.6534666666666666\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 88/100, Iteration: 1/450, Loss: 0.9591259962691219\n",
      "Epoch: 88/100, Iteration: 101/450, Loss: 0.9315634251187762\n",
      "Epoch: 88/100, Iteration: 201/450, Loss: 1.2203721111342347\n",
      "Epoch: 88/100, Iteration: 301/450, Loss: 1.0153803206726386\n",
      "Epoch: 88/100, Iteration: 401/450, Loss: 0.9894018088952802\n",
      "Epoch: 88/100, Iteration: 450/450, Loss: 0.8622360533658463\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 88/100, Train accuracy: 0.7142222222222222, Test accuracy: 0.6616\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 89/100, Iteration: 1/450, Loss: 1.0977683612737952\n",
      "Epoch: 89/100, Iteration: 101/450, Loss: 1.0251334349464496\n",
      "Epoch: 89/100, Iteration: 201/450, Loss: 0.9487995538013653\n",
      "Epoch: 89/100, Iteration: 301/450, Loss: 1.3264821134974398\n",
      "Epoch: 89/100, Iteration: 401/450, Loss: 1.0703135923974882\n",
      "Epoch: 89/100, Iteration: 450/450, Loss: 1.0512952011067973\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 89/100, Train accuracy: 0.7036444444444444, Test accuracy: 0.6614666666666666\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 90/100, Iteration: 1/450, Loss: 1.0723993137522005\n",
      "Epoch: 90/100, Iteration: 101/450, Loss: 1.0816762656391958\n",
      "Epoch: 90/100, Iteration: 201/450, Loss: 1.0043073270000935\n",
      "Epoch: 90/100, Iteration: 301/450, Loss: 0.8492185665943953\n",
      "Epoch: 90/100, Iteration: 401/450, Loss: 1.0140944447232398\n",
      "Epoch: 90/100, Iteration: 450/450, Loss: 1.137693054910387\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 90/100, Train accuracy: 0.7006444444444444, Test accuracy: 0.6463333333333333\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 91/100, Iteration: 1/450, Loss: 0.8598412904770733\n",
      "Epoch: 91/100, Iteration: 101/450, Loss: 1.1611797714487038\n",
      "Epoch: 91/100, Iteration: 201/450, Loss: 0.9923998325050067\n",
      "Epoch: 91/100, Iteration: 301/450, Loss: 1.1681779368064915\n",
      "Epoch: 91/100, Iteration: 401/450, Loss: 0.9054360089024199\n",
      "Epoch: 91/100, Iteration: 450/450, Loss: 1.0620752198043741\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 91/100, Train accuracy: 0.6951555555555555, Test accuracy: 0.6466666666666666\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 92/100, Iteration: 1/450, Loss: 1.0176881355669596\n",
      "Epoch: 92/100, Iteration: 101/450, Loss: 0.9191759691647234\n",
      "Epoch: 92/100, Iteration: 201/450, Loss: 0.946126651776909\n",
      "Epoch: 92/100, Iteration: 301/450, Loss: 1.0034330413246046\n",
      "Epoch: 92/100, Iteration: 401/450, Loss: 0.7951363082104627\n",
      "Epoch: 92/100, Iteration: 450/450, Loss: 0.9718988390145762\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 92/100, Train accuracy: 0.7067777777777777, Test accuracy: 0.6609333333333334\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 93/100, Iteration: 1/450, Loss: 1.0456332011661287\n",
      "Epoch: 93/100, Iteration: 101/450, Loss: 1.1918642768811376\n",
      "Epoch: 93/100, Iteration: 201/450, Loss: 1.0528951185432631\n",
      "Epoch: 93/100, Iteration: 301/450, Loss: 1.0273762721793642\n",
      "Epoch: 93/100, Iteration: 401/450, Loss: 1.045117052979409\n",
      "Epoch: 93/100, Iteration: 450/450, Loss: 0.819831043276287\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 93/100, Train accuracy: 0.7162444444444445, Test accuracy: 0.6620666666666667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 94/100, Iteration: 1/450, Loss: 1.0670133599919163\n",
      "Epoch: 94/100, Iteration: 101/450, Loss: 1.033018409869162\n",
      "Epoch: 94/100, Iteration: 201/450, Loss: 1.0469358091548289\n",
      "Epoch: 94/100, Iteration: 301/450, Loss: 0.8944910693457692\n",
      "Epoch: 94/100, Iteration: 401/450, Loss: 1.0245055198429558\n",
      "Epoch: 94/100, Iteration: 450/450, Loss: 1.10972658000746\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 94/100, Train accuracy: 0.6989333333333333, Test accuracy: 0.6645333333333333\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 95/100, Iteration: 1/450, Loss: 1.0200536193397585\n",
      "Epoch: 95/100, Iteration: 101/450, Loss: 0.9142548600049204\n",
      "Epoch: 95/100, Iteration: 201/450, Loss: 1.1553436025851485\n",
      "Epoch: 95/100, Iteration: 301/450, Loss: 1.051890649391033\n",
      "Epoch: 95/100, Iteration: 401/450, Loss: 1.148558828939267\n",
      "Epoch: 95/100, Iteration: 450/450, Loss: 0.9518970869691499\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 95/100, Train accuracy: 0.7248, Test accuracy: 0.667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 96/100, Iteration: 1/450, Loss: 1.0363648326974486\n",
      "Epoch: 96/100, Iteration: 101/450, Loss: 1.1208845144558204\n",
      "Epoch: 96/100, Iteration: 201/450, Loss: 0.882202695523342\n",
      "Epoch: 96/100, Iteration: 301/450, Loss: 1.140493829214652\n",
      "Epoch: 96/100, Iteration: 401/450, Loss: 1.0817364488251422\n",
      "Epoch: 96/100, Iteration: 450/450, Loss: 0.9616176287144924\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 96/100, Train accuracy: 0.7061333333333333, Test accuracy: 0.655\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 97/100, Iteration: 1/450, Loss: 1.0502894352448207\n",
      "Epoch: 97/100, Iteration: 101/450, Loss: 0.8830140353763839\n",
      "Epoch: 97/100, Iteration: 201/450, Loss: 0.9819257398962901\n",
      "Epoch: 97/100, Iteration: 301/450, Loss: 1.0434598009855942\n",
      "Epoch: 97/100, Iteration: 401/450, Loss: 1.0018995015882368\n",
      "Epoch: 97/100, Iteration: 450/450, Loss: 0.8687210049389578\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 97/100, Train accuracy: 0.7024222222222222, Test accuracy: 0.6587333333333333\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 98/100, Iteration: 1/450, Loss: 0.9380266079682624\n",
      "Epoch: 98/100, Iteration: 101/450, Loss: 0.8970596905465394\n",
      "Epoch: 98/100, Iteration: 201/450, Loss: 0.929972323078191\n",
      "Epoch: 98/100, Iteration: 301/450, Loss: 0.9864834024160704\n",
      "Epoch: 98/100, Iteration: 401/450, Loss: 1.007887358449263\n",
      "Epoch: 98/100, Iteration: 450/450, Loss: 1.0725583694097924\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 98/100, Train accuracy: 0.7019555555555556, Test accuracy: 0.6447333333333334\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 99/100, Iteration: 1/450, Loss: 1.1093660338578386\n",
      "Epoch: 99/100, Iteration: 101/450, Loss: 1.1006771287774504\n",
      "Epoch: 99/100, Iteration: 201/450, Loss: 0.9431274695233968\n",
      "Epoch: 99/100, Iteration: 301/450, Loss: 1.0653422961846564\n",
      "Epoch: 99/100, Iteration: 401/450, Loss: 0.8136863012891626\n",
      "Epoch: 99/100, Iteration: 450/450, Loss: 0.9322815642759125\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 99/100, Train accuracy: 0.7173555555555555, Test accuracy: 0.6652666666666667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 100/100, Iteration: 1/450, Loss: 0.8254086293925778\n",
      "Epoch: 100/100, Iteration: 101/450, Loss: 0.9639313423235959\n",
      "Epoch: 100/100, Iteration: 201/450, Loss: 0.98762979384893\n",
      "Epoch: 100/100, Iteration: 301/450, Loss: 0.915358196551318\n",
      "Epoch: 100/100, Iteration: 401/450, Loss: 1.0801836085074044\n",
      "Epoch: 100/100, Iteration: 450/450, Loss: 0.8974670190886368\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 100/100, Train accuracy: 0.7092, Test accuracy: 0.6500666666666667\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(data={\n",
    "    'X_train': X_train,\n",
    "    'y_train': y_train,\n",
    "    'X_test': X_test,\n",
    "    'y_test': y_test\n",
    "})\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
